{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. User Model \n",
    "In the first part of this notebook, we built up the user model part for our model-based approach to the recommendation problem. The user model consists of two parts -- a state representation model and a reward prediction model. The goal is to train such a model that takes as input a user's historical record $(a_1,r_1, ..., a_{t-1},r_{t-1},a_t)$ and output/predict what the user's feedback r_t will be. Formally, try to train a model $\\hat{r}$ parameterized by $\\theta$ such that $\\hat{r}_\\theta(o_{t-1},a_t) \\approx r(o_{t-1}, a_t)$.\n",
    "\n",
    "### State Representation Model\n",
    "We used a RNN layer to encode historically records of an user, $(a_1,r_1, a_2,r_2,..., a_{t-1},r_{t-1})$, into a fixed-length vector which we call $S_t$. Notice that the size of the input of the RNN (user's historical records) gradually increases over time so we decided to use a RNN to convert it into a fixed-length vector.\n",
    "\n",
    "### Reward Prediction Model\n",
    "After getting our state representation of the user's history, we concatenate it with the user's latest action a_t and feed it into a simple fully-connected layer to output our prediction of $\\hat{r}_t$.\n",
    "\n",
    "### Item/Action Embedding\n",
    "One extra thing we did is to embed each item (or action) in the store into a vector in some high-dimensional space. For example, say we have in total 10,000 items in the repository, and we index each item with a number between 1 and 10,000. When we have a user's historical records and want to feed into the model, we need to feed all those actions $(a_1, a_2, ..., a_{t-1}, a_t)$ into a embedding layer first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to train the user model in a supervised manner with all users' historical records, what we are going to do first is to implement how to do this with *one* user's historical records -- $(a_1, r_1, ..., a_t, r_t)$. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, SimpleRNN, Concatenate, Lambda, Softmax\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from collections import deque\n",
    "from scipy.sparse import coo_matrix\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import random as rn\n",
    "\n",
    "# For strict reproducible purpose\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Preprocess the raw input data from some user $(a_1, r_1, ..., a_t, r_t)$ into a bunch of input-output pairs where input is $(a_1,r_1,...,a_i)$ and output is $r_i$ for all $i$ in $[1,t]$.\n",
    "\n",
    "- We will first slice it into different input-output pairs:\n",
    "    - $a_1 \\rightarrow r_1$\n",
    "    - $a_1, r_1, a_2 \\rightarrow r_2$\n",
    "    - ...\n",
    "    - $a_1, r_1, ..., a_t \\rightarrow r_t$\n",
    "\n",
    "(The next two steps are intended for matching up the input/output shape of the Keras model we are going to build next. Feel free to skip it now :))\n",
    "- Secondly, for each input-output pair, say, $a_1, r_1, ..., a_7, r_7, a_8 \\rightarrow r_8$, I need to extract two inputs and one output from it which will later be fed into the model:\n",
    "    - input1 = $0, a_1, a_2, ..., a_7, a_8$\n",
    "    - input2 = $0, r_1, r_2, ..., r_7$\n",
    "    - output = $r_8$\n",
    "\n",
    "- Finally, convert all those lists (input1, input2, output) into numpy arrays and reshape them to be the correct shape for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the user's observation into strucutred format\n",
    "# that can later be fed into the reward prediction model\n",
    "# to either predict reward or get state representation\n",
    "# (NOTE: this function works fine even if observation is an empty list. This happens\n",
    "#  when we want to get the initial state represenation of the MDP.)\n",
    "def obs2modelinputs(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}] (for state representation purpose)\n",
    "                         or [a_1,r_1,...,a_{t-1},r_{t-1},a_t] (for reward prediction purpose)\n",
    "    \n",
    "    return: observation's corresponding structured data that can be directly fed into the reward prediction model\n",
    "    '''\n",
    "    \n",
    "    if len(observation) % 2 == 0:\n",
    "        # To get state representation, we simply append a 0 to observation as a 'fake' a_t\n",
    "        # This is okay since we won't use a_t anyway when computing the state representation of observation\n",
    "        action_inputs = np.array([0] + observation[::2] + [0]) \n",
    "    else:\n",
    "        action_inputs = np.array([0] + observation[::2])\n",
    "    \n",
    "    reward_inputs = np.array([0] + observation[1::2])\n",
    "    \n",
    "    # Reshape inputs so that the reward prediction model\n",
    "    # or the state representation model can accpet\n",
    "    action_inputs = action_inputs.reshape(1,-1)\n",
    "    reward_inputs = reward_inputs.reshape(1,-1)\n",
    "    \n",
    "    return action_inputs, reward_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct training set -- a list of training samples obs_{t-1},a_t --> r_t, t = {1,2,...T}\n",
    "# in strucutured format -- from one user's current historical records [a_1,r_1,...,a_T,r_T]\n",
    "# The return can directly be sampled one at a time and feed into the reward prediction for training\n",
    "def construct_strucutred_training_set(observation):\n",
    "    '''\n",
    "    param:\n",
    "    observation: a list of this user's interactions with the recommendation system\n",
    "                 [a_1,r_1,...,a_T,r_T]\n",
    "    \n",
    "    return:\n",
    "    structured_training_set: a list of training samples where each element in this list is\n",
    "    a tuple (inputs, outputs) for that training sample.\n",
    "    '''\n",
    "    \n",
    "    assert len(observation)%2 == 0, 'Please make sure the observation on which \\\n",
    "                                     you want to construct training set has even number of elements.'\n",
    "    \n",
    "    T = len(observation) // 2\n",
    "    # training_set stores all the training samples sliced from the observation\n",
    "    # each training sample in it is a tuple ([a_1,r_1,...,a_t], r_t) for t = {1,2,...T}\n",
    "    training_set = [] \n",
    "    for t in range(T):\n",
    "        x = observation[0 : 2 * t + 1] # a list   [a_1,r_1,...a_t]\n",
    "        y = observation[2 * t + 1]     # a scalar r_t\n",
    "        training_set.append((x, y))\n",
    "    \n",
    "    # Convert each training sample in training_set into structured format\n",
    "    structured_training_set = []\n",
    "    for sample in training_set:\n",
    "        x = sample[0] # [a_1,r_1,...,a_t]\n",
    "        y = sample[1] # r_t\n",
    "        \n",
    "        action_inputs, reward_inputs = obs2modelinputs(x)\n",
    "        # Do a similar thing to output/r_t\n",
    "        reward_output = np.array(y).reshape(1,1)\n",
    "        \n",
    "        structured_training_set.append(([action_inputs, reward_inputs], [reward_output]))\n",
    "    \n",
    "    return structured_training_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator function \n",
    "it generats one batch of training samples at a time for the *reward prediction model* to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_sample_generator(structured_training_set):\n",
    "    '''\n",
    "    params: \n",
    "    structured_data: Return of the construct_strucutred_training_set function. \n",
    "    '''\n",
    "    while True:\n",
    "        # Randomly yield one training sample from the training set without repetition\n",
    "        # before consuming all training samples and starting a new round\n",
    "        np.random.shuffle(structured_training_set)\n",
    "        for training_sample in structured_training_set:\n",
    "            yield training_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building-up layers for the user model\n",
    "We will define the necessary layers required for our user model. \n",
    "Later, we will build two models, reward prediction model and policy model, from those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 69878 # total number of users -- used in the environment simulator\n",
    "num_items = 10677 # total number of items -- cardinality of action space |A|\n",
    "dimension_action_embedding = 50\n",
    "dimension_state_representation = 10\n",
    "gamma = 0.99 # Discounting factor\n",
    "\n",
    "r_low = 0\n",
    "r_high = 5\n",
    "dimension_reward_ohe = 10\n",
    "\n",
    "# Three functions that will later be used to construct Lambda layers\n",
    "def tminusone_actions(action_embeddings):\n",
    "    # action_embeddings.shape = (batch_size, time_steps, dimension_action_embedding)\n",
    "    return action_embeddings[:,:-1,:]\n",
    "\n",
    "def action_t(action_embeddings):\n",
    "    # action_embeddings.shape = (batch_size, time_steps, dimension_action_embedding) \n",
    "    return action_embeddings[:,-1,:]\n",
    "\n",
    "def one_hot(reward_inputs):\n",
    "    # reward_inputs.shape = (batch_size, time_steps)\n",
    "    # We implicitly use broad-casting here since dimension_reward_ohe, r_low, r_high are all scalars\n",
    "    indices = dimension_reward_ohe - (dimension_reward_ohe * (r_high - reward_inputs)) // (r_high - r_low)\n",
    "    \n",
    "    # indices is a rank-2 tensor whose datat type is floating point although each\n",
    "    # element in indices is already integer (e.g. 3.0, 8.0, 9.0, etc.). We thus cast\n",
    "    # it to integer data type so that one_hot function call be evoked correctly\n",
    "    indices = tf.dtypes.cast(indices, 'int32')\n",
    "    \n",
    "    # shape of return = (batch_size, time_steps, dimension_reward_ohe)\n",
    "    return K.one_hot(indices, dimension_reward_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Input Layers (notice the slightly asymmetry b/t these two inputs)\n",
    "action_inputs = Input(shape=(None,), name='action_inputs', dtype='int32') # 0,a_1,a_2,...,a_{t-1},a_t. 'None' here means any t can be expected\n",
    "reward_inputs = Input(shape=(None,), name='reward_inputs', dtype='float32') # 0,r_1,r_2,...,r_{t-1}.     'None' here means any t can be expexted\n",
    "\n",
    "\n",
    "# Embedding layer for actions\n",
    "# input_dim = num_items + 1 because items are ranked from 1 to num_items. We reserve 0 for the dummy action\n",
    "action_embeddings = Embedding(input_dim=num_items+1, output_dim=dimension_action_embedding, name='action_embeddings')(action_inputs)\n",
    "\n",
    "# Split the output to two parts -- embeddings for (0,a_1,...,a_{t-1}) and embedding for a_t\n",
    "tminusone_action_embeddings = Lambda(tminusone_actions)(action_embeddings)\n",
    "action_t_embedding = Lambda(action_t)(action_embeddings)\n",
    "\n",
    "# One-hot encoding reward inputs\n",
    "t_minusone_reward_ohes = Lambda(one_hot)(reward_inputs)\n",
    "\n",
    "# RNN layer for state representation\n",
    "rnn_inputs = Concatenate(axis=-1)([tminusone_action_embeddings, t_minusone_reward_ohes])\n",
    "state_representation = SimpleRNN(dimension_state_representation, name='state_representation')(rnn_inputs)\n",
    "\n",
    "\n",
    "# A dense layer for reward prediction\n",
    "reward_prediction_inputs = Concatenate(axis=-1)([state_representation, action_t_embedding])\n",
    "reward_prediction = Dense(1)(reward_prediction_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward Prediction Model\n",
    "The reward prediction model consist of an action/item embedding layer + a RNN (aka state representation layer) + a reward prediction model (fully-connected layer).\n",
    "\n",
    "The inputs of training data for this model are of various length and we have only one training sample for each length. Thus, we feed each training sample to the model and train the network on each sample (like stochastic gradient descent). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward prediction model\n",
    "reward_prediction_model = Model(inputs=[action_inputs, reward_inputs], outputs=reward_prediction)\n",
    "\n",
    "\n",
    "# 'Freeze' the state-representation model for training the\n",
    "# state-value prediction model and the policy model.\n",
    "# NOTICE: we freeze the state representation weights only because\n",
    "# we want to use the same initial weights (trained and saved by\n",
    "# the model-free solution) for both model-free and model-based solution\n",
    "reward_prediction_model.get_layer('action_embeddings').trainable = False\n",
    "reward_prediction_model.get_layer('state_representation').trainable = False\n",
    "\n",
    "\n",
    "# Model compilation: 'rmsprop' is recommended for rnn and 'mse' is used \n",
    "# because we have a regression problem\n",
    "reward_prediction_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "# Train the reward prediction model on user's observation \n",
    "def train_reward_prediction_model(observation, steps_per_epoch=1000, epochs=10):\n",
    "    '''\n",
    "    params: observation: a list of user's current historical records [a_1,r_1,...,a_t,r_t]\n",
    "            steps_per_epoch: steps to train in each epoch\n",
    "            epochs: number of total training epochs\n",
    "    '''￼\n",
    "    structured_training_set = construct_strucutred_training_set(observation)\n",
    "    reward_prediction_model.fit_generator(training_sample_generator(structured_training_set), steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=0)\n",
    "\n",
    "# Predict the user's feedback/reward for action based on his/her historical records (observation)\n",
    "def predict_reward(observation, action):\n",
    "    '''\n",
    "    params: observation: a list of user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "            action: the action just taken a_t (a scalar)\n",
    "            \n",
    "    return: predicted reward for the item just recommended \\hat{r}_t (a scalar)\n",
    "    '''\n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation + [action])\n",
    "    return reward_prediction_model.predict([action_inputs, reward_inputs])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Representation Model\n",
    "It turns out that we are going to need to compute the current state $S_t$ given the user's current historical record $O_{t-1}$. Thus, let's define a state representation model which we can conveniently use to encode history into states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well, this state representation model is not necessary.\n",
    "# We still define such a model for debugging convenience\n",
    "# since we might want to take a look at what the state \n",
    "# representation is like.\n",
    "state_representation_model = Model(inputs=reward_prediction_model.input, outputs=reward_prediction_model.get_layer(name='state_representation').output)\n",
    "\n",
    "def get_state_representation(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's historica records to be encoded [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "    \n",
    "    return: an encoded vector representing S_t. Shape = (1, dimension_state_representation).\n",
    "    '''\n",
    "    assert len(observation) % 2 == 0, 'historical records to be encoded to a state must have even length.'\n",
    "    \n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    return state_representation_model.predict([action_inputs, reward_inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity Check 1\n",
    "A sanity check to see whether the code is working by playing with some toy contrived data.\n",
    "\n",
    "Well, at least I saw two things:\n",
    "    1. The program can run without complilation errors;\n",
    "    2. The reward prediction model overfits the toy dataset;\n",
    "\n",
    "There is one things I don't know how to verify:\n",
    "    1. How to verify we learned a good state representaion? Given an observation $O_{t-1}$, I can feed it to the state representation network and print out the result but I don't know how to intepret those numbers. How can I somehow find a way to justify that indeed those state representation is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_observation = [123,5, 12,8, 291, 10, 244, 4, 2521,1]\n",
    "print('State representation of observation before training the state representation model')\n",
    "print(get_state_representation(partial_observation))\n",
    "print('Now take action 5771')\n",
    "print('Based on this state representation, our prediction is: ')\n",
    "print(predict_reward(partial_observation, 5771))\n",
    "\n",
    "observation = [123,5, 12,8, 291, 10, 244, 4, 2521,1, 5771,10, 6811,9, 2349, 2, 19, 1, 52,5, 122,5, 1272,8, 99,6]\n",
    "train_reward_prediction_model(observation)\n",
    "\n",
    "\n",
    "print('State representation of observation after training the state representation model')\n",
    "print(get_state_representation(partial_observation))\n",
    "print('Based on this state representation, our prediction is: ')\n",
    "print(predict_reward(partial_observation, 5771))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-step Actor-Critic to Improve our recommendation policy\n",
    "Now we are going to build up the policy network. After receiving each transition $(S_t,A_t,R_t,S_{t+1})$, we can use Actor-Critic algorithm (one-step) to update our policy network as well as our state-value prediction network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the *One-step Actor-Critic* algorithm in RL to update the policy network. Namely, the algorithm requires the policy to chose an action $A_t$ in a given state $S_t$ (computed from $O_{t-1}$). Take this action and observe reward $R_t$ and next state $S_{t+1}$ (computed from $O_t$). Next we compute the temporal-difference error $\\delta=R_t + \\gamma*\\hat{v}(S_{t+1};w) - \\hat{v}(S_t;w)$, where $\\hat{v}(S;w)$ is a network (function approximator) parameterized by $w$ that tries to predict state-values. With this TD error, we can update the state-value prediction network and later the policy network. The details of how to update those two networks are discussed later. Now let's first try to build up a state-value prediction network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-value Prediction Network\n",
    "We implement a simple fully connected layer to predict the state-value of a given state encoded from observation: $\\hat{v}(O_{t-1};w) \\approx v(S_t)$.\n",
    "\n",
    "#### Training the State-value Prediction Network with One-Step Transition\n",
    "After receiving the one-step transition $(S_t,A_t,R_t,S_{t+1})$, we train/update the state-value prediction network as follows:\n",
    "- first compute the temporal-difference error $\\delta = R_t + \\gamma*\\hat{v}(S_{t+1};w)$;\n",
    "- secondly, fit the model with (one) training sample $(O_{t-1}, \\delta)$ using 'mean_squared_error' loss. Furthermore, I choose 'sgd' optimizer since it is simple and we will be training this state-value prediction network with only one transition tuple anyway, meaning batch_size=1, so it does not make sense to use advanced optimizer which will only have an effect when there are more than more training sample in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up fully connected layers (here it is just one layer) \n",
    "# after the state representation network to predict state-values\n",
    "# of the state representation\n",
    "state_value_prediction = Dense(1)(state_representation)\n",
    "\n",
    "# # 'Freeze' the state-representation model for training the\n",
    "# # state-value prediction model and the policy model\n",
    "# reward_prediction_model.get_layer('action_embeddings').trainable = False\n",
    "# reward_prediction_model.get_layer('state_representation').trainable = False\n",
    "\n",
    "\n",
    "state_value_prediction_model = Model(inputs=[action_inputs, reward_inputs], outputs=state_value_prediction)\n",
    "state_value_prediction_model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "\n",
    "def predict_state_value(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's historica records to be encoded [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "    \n",
    "    return: predicted state value of the encoded state from observation (a scalar)\n",
    "    '''\n",
    "    assert len(observation) % 2 == 0, 'historical records to be encoded to a state must have even length.'\n",
    "    \n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    return state_value_prediction_model.predict([action_inputs, reward_inputs])[0][0]\n",
    "    \n",
    "    \n",
    "def train_state_value_prediction_model_with_one_step_transition(observation, action, reward):\n",
    "    '''\n",
    "    params: observation: the user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "            action: the action a_t just taken by the recommendation system (a scalar)\n",
    "            reward: user's feedback on the action/item just taken (a scalar)\n",
    "    '''\n",
    "    TD_error = reward + gamma * predict_state_value(observation + [action, reward])\n",
    "    \n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    # Reshape TD_error so that it can be fed to model.fit function\n",
    "    TD_error = np.array(TD_error).reshape(1,-1)\n",
    "    \n",
    "    state_value_prediction_model.fit([action_inputs,reward_inputs], TD_error, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity Check 2\n",
    "This sanity check whether the state-value prediction model is working or not. Again, a same problem here is that we don't know how to verify the training results. We don't even know the ground truth $v_\\pi(S)$. In the Actor-Critic, this state value prediction model is supposed to learn state-values of current policy (the Actor).\n",
    "Nevertheless, we can see that our code is at least bug-free with this sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = [123,5, 12,8, 291, 10, 244, 4, 2521,1, 5771,10, 6811,9, 2349, 2, 19, 1, 52,5, 122,5, 1272,8, 99,6]\n",
    "\n",
    "ob = []\n",
    "print('ob = [], with initial weights:')\n",
    "print('state representation of ob = ' + str(get_state_representation(ob)))\n",
    "print('predicted state value for this state representation = ' + str(predict_state_value(ob)))\n",
    "\n",
    "print('\\nnow we take action 123 and receive reward 5, and we train the state prediction model with this one-step transition')\n",
    "train_state_value_prediction_model_with_one_step_transition(ob, 123, 5)\n",
    "print('training finished!')\n",
    "\n",
    "ob = [123, 5]\n",
    "print('\\nob = [123, 5]')\n",
    "print('state representation of ob = ' + str(get_state_representation(ob)))\n",
    "print('predicted state value for this state representation = ' + str(predict_state_value(ob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network\n",
    "Now we are ready to build up the policy $\\pi_\\theta(A|S)$ -- a network, parameterized by $\\theta$ that takes the user's current state $S$ as input and output probabilities of choosing all possible actions $\\pi(A|S)$.\n",
    "\n",
    "We are gonna re-use the state representation network to encode observations into states for us, and we connect the output of the state repersentation network to two layers: one dense layer that computes the *preferences* for each action in this state followed by a *soft-max* layer that normalizes these preferences into probabilities.\n",
    "\n",
    "The preference $h(S,A)$ for each possible action is computed through a linear combination of the state representation $h(S,A) = w_A^Tx(S)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences = Dense(num_items, name='preferences')(state_representation)\n",
    "probabilities = Softmax(name='probabilities')(preferences)\n",
    "\n",
    "policy_model = Model(inputs=[action_inputs, reward_inputs], outputs=probabilities)\n",
    "policy_model.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample an Action from the Policy Network\n",
    "Given a state representation $S_t$, which is encoded from observation $O_{t-1}$, we can do a forward pass to compute the probabilities of each action under current policy network (output of policy_model). Then we sample an action [1, num_items] from this probabilistic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probabilities of taking each action under \n",
    "# current (stochastic) policy\n",
    "def predict_policy_probabilities(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's current historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "    \n",
    "    return: probabilities: a numpy array of size num_items where each element corresponds to the \n",
    "                           probability of choosing that action under current policy\n",
    "    '''\n",
    "    assert len(observation) % 2 == 0, 'historical records to be encoded to a state must have even length.'\n",
    "    \n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    probabilities = policy_model.predict([action_inputs, reward_inputs])[0]\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "# Take an action from the (stochastic) policy\n",
    "def sample_action_from_policy(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's current historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "    \n",
    "    return: an action A_t to take according to the current policy network. (a scalar)\n",
    "    '''\n",
    "    probabilities = predict_policy_probabilities(observation)\n",
    "    sampled_action = np.random.choice(np.arange(1, num_items+1), p=probabilities)\n",
    "    \n",
    "    return sampled_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Policy Network with One-step Transition\n",
    "According to the One-step Actor-Critic (Episodic) algorithm, the policy update formula is $$\\theta \\leftarrow \\theta + \\alpha \\gamma^t (R_t + \\gamma\\hat{v}_w(S_{t+1}) - \\hat{v}_w(S_t)) \\nabla_\\theta(\\ln\\pi_\\theta(A_t|S_t))$$\n",
    "\n",
    "We made two somewhat unreasonable modifications for practical reasons:\n",
    "1. Strictly speaking, this is an algorithm for episodic case and our recommendation problem is a continuing problem per se, and this update is for episodic problems. But nevertheless, we will still use it because the Action-Critic algorithm for continuing problem on the textbook uses average reward, something I don't think I have a good understanding. So we will still use this algorithm despite it might not be a perfect match.\n",
    "2. We will also get rid of the $\\gamma^t$ term. This changes the estimate of the gradient of the objective function ($J(\\theta)$) an biased one but we will live with it, because otherwise, as t goes up, we are afraid that the update will be scaled too small such that no substantial change will be made to the parameters $\\theta$.\n",
    "\n",
    "Thus, the update for our policy model becomes \n",
    "$$ \\delta = R_t + \\gamma\\hat{v}_w(S_{t+1}) - \\hat{v}_w(S_t) $$\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta(\\ln\\pi_\\theta(A_t|S_t))$$\n",
    "\n",
    "Then, our job is to design a loss as well as an appropriate input-output from $(S_t,A_t,R_t,S_{t+1})$ such that when keras computes the gradients of the network's parameters $\\theta$, it should equal to $\\delta \\nabla_\\theta(\\ln\\pi_\\theta(A_t|S_t))$.\n",
    "We use $(x,y) = (S_t, \\delta \\text{one_hot}(A_t))$ as our training sample (we train the network on only one sample at a time which is constructed from the latest transition) and use 'categorical_crossentropy' loss: \n",
    "$$ -\\sum_{i=1}^{|A|}y_i \\ln \\hat{y}_i = - \\sum_{i=1}^{|A|} \\delta \\text{one_hot}(A_t)_i \\ln \\pi_\\theta(A_i|S_t) = - \\delta \\ln \\pi_\\theta(A_t|S_t)$$\n",
    "\n",
    "Finally, when we call model.fit() with one training sample $(x,y)$ fed in, it will automatically compute the gradient of this loss w.r.t. $\\theta$ \n",
    "$$ \\nabla_\\theta ( - \\delta \\ln \\pi_\\theta(A_t|S_t)) = - \\delta \\nabla_\\theta (\\ln \\pi_\\theta(A_t|S_t)) $$\n",
    "and update $\\theta$ in the opposite direction of the gradient (in order to minimize the loss)\n",
    "$$ \\theta \\leftarrow \\theta - \\alpha (- \\delta \\nabla_\\theta (\\ln \\pi_\\theta(A_t|S_t))) \\\\ \\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta (\\ln \\pi_\\theta(A_t|S_t)) $$\n",
    "which is exactly what we want! Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_model_with_one_step_transition(observation, action, reward):\n",
    "    '''\n",
    "    params: observation: the user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "            action: the action a_t just taken by the recommendation system (a scalar)\n",
    "            reward: user's feedback on the action/item just taken (a scalar)\n",
    "    '''\n",
    "    assert len(observation) % 2 == 0, 'historical records to be encoded to a state must have even length.'\n",
    "    \n",
    "    predicted_value_state = predict_state_value(observation)\n",
    "    predicted_value_next_state = predict_state_value(observation + [action, reward])\n",
    "    \n",
    "    TD_error = reward + gamma * predicted_value_next_state - predicted_value_state\n",
    "    \n",
    "    # create an one-hot encoding of action.\n",
    "    # Do not confused it with the hot_encoding lambda layer.\n",
    "    # Here it is just a trivial hot encoding of action.\n",
    "    one_hot_encoding_action = np.zeros(num_items).reshape(1,-1) # Shape = (1,num_items)\n",
    "    one_hot_encoding_action[0][action-1] = 1 # [action-1] because actions are indexed from 1 to num_items\n",
    "    \n",
    "    # a strange target, isn't it? See the documentation for more details!\n",
    "    target = TD_error * one_hot_encoding_action\n",
    "    \n",
    "    # retrieve inputs\n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    \n",
    "    policy_model.fit([action_inputs, reward_inputs], target, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check 3\n",
    "Play with the policy network with contrievd data. Namely, we create a 'fake' environment simulator that returns reward given state and action. We then update the policy network with fixed state-value prediction network (with its initial weights) after receiving each transition tuple.\n",
    "\n",
    "What I saw from the result is that, despite a very very bad state-value prediction network, the policy network converges to a deterministic policy and outputs the same action no matter what current state is. This is reasonable because the way how we set up the simulated environment does not use any state information. One thing I don't know whether it's reasonable or not is that the algorithm does not find the exact optimal action, which should be num_items//2 in our contrived example, but rather an item near it. Furthermore, the action this algorithm converges to is not the same between different trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fake environment simulator\n",
    "def simulated_reward(ob, action):\n",
    "    # a Gaussian-like distribution of reward over items\n",
    "    # Items with intermediate index (~num_items//2) have largest reward\n",
    "    return scipy.stats.norm(num_items // 2, 1000).pdf(action) * 10000\n",
    "    \n",
    "ob = deque(maxlen=10)\n",
    "\n",
    "for t in range(30000):\n",
    "    print('\\nt = ' + str(t))\n",
    "    action = sample_action_from_policy(list(ob))\n",
    "    print('a' + str(t+1) + ' = ' + str(action))\n",
    "    reward = simulated_reward(list(ob), action)\n",
    "    print('r' + str(t+1) + ' = ' + str(reward))\n",
    "    print('Update policy')\n",
    "    train_policy_model_with_one_step_transition(list(ob), action, reward)\n",
    "    ob += [action, reward]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Step Actor-Critic Algorithm\n",
    "We have built all the components needed to implement the one-step Actor-Critic algorithm. We will now integrate them into one single function `one_step_actor_critic` that takes as input one transition tuple $(S_t, A_t, R_t, S_{t+1})$ (well, actually $(O_{t-1}, A_t, R_t)$ since $S_t$ can be encoded from $O_{t-1}$ and $S_{t+1}$ can be encoded from $O_t = [O_{t-1},A_t,R_t]$). This function does two things: update the state-value prediction network (Critic) and update the policy network (Actor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_actor_critic(observation, action, reward):\n",
    "    '''\n",
    "    params: observation: the user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "            action: the action a_t just taken by the recommendation system (a scalar)\n",
    "            reward: user's feedback r_t on the action a_t (a scalar)\n",
    "    \n",
    "    '''\n",
    "    train_state_value_prediction_model_with_one_step_transition(observation, action, reward)\n",
    "    train_policy_model_with_one_step_transition(observation, action, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Simulator\n",
    "Now we have every component done and are ready to test our model/algorithm with experiment. Since our algorithm concerns the problem of learning a good recommendation policy via interaction with the user, we will try to test how good the performance of our model/algorithm is by measuring the learning process with interactive recommendation experience with user. However, one big problem is that it is commercially impractical to test our algorithm by putting our recommendation model/algorithm online and let it interact with real customers. Therefore, we build an environment simulator that tries to mimic the customer's feedback/reward on recommended items based on public recommendation dataset. This environment simulator takes a user_id (an int) and a item_id (an int) as input and returns the user's feedback (a float) on this item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset -- MovieLens 10M\n",
    "Here we briefly discuss the dataset we used to build the environment simulator. We used the *MovieLens 10M Dataset* which contains 10,000,054 ratings for 10677 movies from 69878 users. The ratings range from 0.0 (exclusive) to 5.0 (inclusive) with an increment of 0.5. It also contains other information like tags of movies and time stamps but we will only use the ratings information (i.e. ratings for different movies from different users).\n",
    "\n",
    "For example, among all those 10,000,054 ratings, one rating is like 46::152::3.5, which stands for user_46 gave a 3.5 to movie_152. Both the index of users and movies start from 1 to 69878 and 10677 respectively. \n",
    "\n",
    "On average, each user has 143 ratings (to 143 different movies), and each movie is rated by 936 users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Up a Simulator from MovieLens 10M\n",
    "The idea is simple: we are going to create a large rating table whose rows represents user ids (from 1 to 69878) and columns represents movie ids (from 1 to 10677). If the dataset contains a rating $r$ for user_i to movie_j, then the $(i,j)$ entry of the table is $r$. We are also going to normalize the ratings from $(0.0, 5.0]$ to $(-1,1)$ while maintaining ratio:\n",
    "$$new\\_rating = \\frac{2}{5} * old\\_rating  - 1$$\n",
    "And for those entries without actual rating in the dataset, we thus set it to 0.0, implying a neutral rating in the normalized range. So, you can see that this table is in fact a large sparse table with a lot of 0.0's. Well, there is nothing we can do about this since the dataset contains only this much ratings to fill a small part of the rating table.\n",
    "\n",
    "One little extra thing is that in the original dataset, the ids of the users and movies are not indexed with {1,2,3,...} but might have gaps. That is, the actual index of users can be something like {1,2,3,5,6,9,10,...} so you might see a rating record from user_id 71567 although there are in total just 69878 users. Hence, I also reordered the user and movie index from {1,2,3,...} up 7to either 69878 for user_ids or 10677 for movie_ids through a simple mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/home/xiang/Desktop/Graduation_Thesis/ratings.dat'\n",
    "data = np.loadtxt(data_path, delimiter='::')\n",
    "\n",
    "rating_range_low = 0.0\n",
    "rating_range_high = 5.0\n",
    "new_rating_range_low = -1.0\n",
    "new_rating_range_high = 1.0\n",
    "# new_data = a * old_data + b transforms data from one range\n",
    "# to another, which must be symmetric around 0, while maintaining ratio\n",
    "assert new_rating_range_low + new_rating_range_high == 0, 'Normalized range must be symmetric around 0'\n",
    "a = (new_rating_range_high - new_rating_range_low) / (rating_range_high - rating_range_low)\n",
    "b = new_rating_range_low - a * rating_range_low      \n",
    "\n",
    "user_ids = set()\n",
    "movie_ids = set()\n",
    "\n",
    "for user_id, movie_id, rating, time_step in data:\n",
    "    user_ids.add(int(user_id))\n",
    "    movie_ids.add(int(movie_id))\n",
    "\n",
    "# Create the user ids mapping\n",
    "# each key-value pair is (new_index : old_index_in_data)\n",
    "user_ids_mapping = dict()\n",
    "i = 1\n",
    "for user_id in user_ids:\n",
    "    user_ids_mapping[i] = user_id\n",
    "    i += 1 \n",
    "\n",
    "# Similarly, create the movie ids mapping\n",
    "movie_ids_mapping = dict()\n",
    "i = 1\n",
    "for movie_id in movie_ids:\n",
    "    movie_ids_mapping[i] = movie_id\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Create the rating table based on the (original) ratings\n",
    "# in the dataset\n",
    "rating_table = coo_matrix((data[:, 2], (data[:, 0].astype(int), data[:, 1].astype(int)))).todok()\n",
    "\n",
    "\n",
    "def get_rating(user_id, movie_id):\n",
    "    '''\n",
    "    params: user_id: index of the user (an int from 1 to num_users inclusively)\n",
    "            movie_id: index of the movie (an int from 1 to num_items inclusively)\n",
    "    \n",
    "    return: normalized simulated rating for this user_id to movie_id (a float b/t (-1, 1])\n",
    "    '''\n",
    "    old_rating = rating_table[user_ids_mapping[user_id], movie_ids_mapping[movie_id]]\n",
    "    \n",
    "    if old_rating == 0.0:\n",
    "        return old_rating\n",
    "    else:\n",
    "        # normalize the rating to (-1, 1]\n",
    "        return a * old_rating + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting All Together into the Dyna Architecture\n",
    "Congrats on making to this point. We finished all the crucial components and one last thing is to put them together into the Dyna architecture.\n",
    "\n",
    "This idea of Dyna architecture is model-based RL -- it constructs/learns a model of the environment dynamics and tries to learn from both real experience as well as imaginary experience generated from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Dyna algorithm with user_id for max_iterations (T) rounds\n",
    "# where within each round do the planning for max_planning_steps steps\n",
    "def Dyna(user_id=1, max_iterations=100, max_planning_steps=100):\n",
    "    '''\n",
    "    params: user_id: the id of the user we are going to interact with in the (simulated)\n",
    "                     environment. (an int between 1 and num_users inclusively)\n",
    "            max_iterations: max number of iterations of the dyna algorithm (an int scalar)\n",
    "            max_planning_steps: max number of steps allowed in the planning step\n",
    "                                in the dyna algorithm (an int scalar)\n",
    "                                \n",
    "    return: observation: the interactive history (A_1,R_1,...,A_T,R_T), which will\n",
    "                         be stored in a replay buffer and later used to periodically\n",
    "                         retrain our user model.\n",
    "    '''\n",
    "    observation = []\n",
    "    \n",
    "    for t in range(max_iterations):\n",
    "        # Choose an action to take from our policy model\n",
    "        # and observe its reward\n",
    "        action = sample_action_from_policy(observation)\n",
    "        reward = get_rating(user_id, action)\n",
    "\n",
    "        # Perform One-step Actor-Critic update with the latest transition\n",
    "        one_step_actor_critic(observation, action, reward)\n",
    "\n",
    "        observation += [action, reward]\n",
    "        \n",
    "        # Enter the planning module\n",
    "        rollout_observation = deepcopy(observation)\n",
    "        for _ in range(max_planning_steps):\n",
    "            # Choose an action to take from our policy model\n",
    "            # and query our reward prediction model for estimated reward\n",
    "            action = sample_action_from_policy(rollout_observation)\n",
    "            reward = predict_reward(rollout_observation, action)\n",
    "\n",
    "            # Perform One-step Actor-Critic update with the latest imaginary transition\n",
    "            one_step_actor_critic(rollout_observation, action, reward)\n",
    "\n",
    "            rollout_observation += [action, reward]\n",
    "\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "Finally, we are going to conduct some experiment with all the components we had built so far. From a high level, the experiment involves two parts:\n",
    "1. Interact with the enviroment and learn a good recommendation policy;\n",
    "2. Evaluate the learning process of our algorithm/model\n",
    "\n",
    "We will go through each of the two parts in further detailed subparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learn a Recommendation Policy through Interaction\n",
    "We can break down this task further into three steps:\n",
    "\n",
    "1. Pre-train a user model. We initialize a (random) policy $\\pi_\\theta(S,A)$ and use it to interact with the (simulated) environment and collect a set of interactive data $\\{(A_1^{(1)},R_1^{(1)},\\dots,A_T^{(1)},R_T^{(1)}), (A_1^{(2)},R_1^{(2)},\\dots,A_T^{(2)},R_T^{(2)}), \\dots, (A_1^{(K)},R_1^{(K)},\\dots,A_T^{(K)},R_T^{(K)}) \\}$, where $A_t^{(k)}$ represents the action taken in time step $t$ for user $k$ and $R_t^{(k)}$ accordingly stands for the reward/feedback from user $k$ for the recommended item $A_t^{(k)}$. We then use this dataset to train a user model. (K here is a hyperparameter)\n",
    "\n",
    "2. Run the Dyna algorithm to learn a good recommendation policy. Start by randomly picking up an user and run the dyna algorithm with this user (`user_id`) for several steps (`max_iterations`, `max_planning_steps`). Add the return value (a list of (real) interactive history of this user) to a replay buffer, which is a fixed length deque of lists. Then we repeat the whole process by randomly selecting another user. We repeat this process until `buffer_ratio_retrain` ((0, 1] e.g. 1/3) data in the replay buffer is replaced by the new ones. Then we go to step 3.\n",
    "\n",
    "3. Periodically retrain user model. Recall that we maintain a replay buffer (of fixed size) of interactive experiences. We will use the data in this buffer to retrain our user model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_user_model_from_replay_buffer(replay_buffer):\n",
    "    '''\n",
    "    params: replay_buffer: a queue of interactive experience of same length.\n",
    "    '''\n",
    "    \n",
    "    # Check for precondition -- make sure all elements in replay_buffer have the same length\n",
    "    if len(replay_buffer) != 0:\n",
    "        for idx in range(1, len(replay_buffer)):\n",
    "            if len(replay_buffer[idx]) != len(replay_buffer[0]):\n",
    "                assert False, 'historical experience in replay_buffer must have same length!'\n",
    "    \n",
    "    length_episode = len(replay_buffer[0]) // 2\n",
    "    \n",
    "    # Randomly select observations from replay_buffer without repetition\n",
    "    # and train our user model on each observation. Since we don't want to\n",
    "    # modify replay_buffer in-place, we create an auxiliary list for replay_buffer\n",
    "    aux_replay_buffer = list(replay_buffer)\n",
    "    np.random.shuffle(aux_replay_buffer)\n",
    "    \n",
    "    for ob in aux_replay_buffer:       \n",
    "        train_reward_prediction_model(ob, steps_per_epoch=length_episode, epochs=1)\n",
    "    \n",
    "\n",
    "# Pre-train our user model by interacting with all users using\n",
    "# an initial policy. This function does two things: fill in a \n",
    "# replay buffer and train our user model (aka reward prediction model)\n",
    "# with data in the replay buffer\n",
    "def pre_train_user_model(num_users_to_interact, length_episode):\n",
    "    '''\n",
    "    params: num_users_to_interact: number of users we are going to interact with (an int >= 1)\n",
    "            length_episode: length of the interaction with each user (an int >= 1)\n",
    "            \n",
    "    return: a queue of length num_users_to_interact where each element is the interactive \n",
    "            history of that user.\n",
    "    '''\n",
    "    replay_buffer = deque(maxlen=num_users_to_interact)\n",
    "    \n",
    "    # Fill in the replay buffer\n",
    "    for idx in range(num_users_to_interact):\n",
    "        # Randomly select an user. Note that we index users from 1 to num_users!\n",
    "        user_id = np.random.randint(num_users) + 1\n",
    "    \n",
    "        # Use the initial policy to interact with user user_id for length_episode\n",
    "        # steps without improving the policy or anything -- just collect experiences\n",
    "        ob = []\n",
    "        for _ in range(length_episode):\n",
    "            action = sample_action_from_policy(ob)\n",
    "            reward = get_rating(user_id, action)\n",
    "            ob += [action, reward]\n",
    "        \n",
    "        # Add this user's experience (ob) to replay buffer\n",
    "        replay_buffer.append(ob)\n",
    "    \n",
    "    print('Fill in replay buffer finished!\\n')\n",
    "    \n",
    "    # Train user model from the replay buffer\n",
    "    train_user_model_from_replay_buffer(replay_buffer)\n",
    "    \n",
    "    print('Train on replay buffer finished!\\n')\n",
    "    \n",
    "    return replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the Learning Process\n",
    "Every once in a while, we need to evaluate our (recommendation) policy to see how good it current is. We are going to implement a procedure which can measure the performance of our policy using four metrics -- average reward, precision@k, recall@k, and F1@k.\n",
    "\n",
    "Concretely, for **average reward**, here is how we are going to do it:\n",
    "1. Randomly select an user $i$ from 1 to num_users;\n",
    "2. Interact with this user (in the simulator) for certain steps (episode length e.g. 32) using current policy and record the rewards $R_1, R_2,\\dots,R_{T}$;\n",
    "3. Average of these $T$ rewards to calculate the average reward $\\bar{R}^{(i)}$ for this user under current policy;\n",
    "4. Go back to step 1 and repeat for multiple users, and average over all these average rewards $\\bar{R}^{(i)}$'s to get the overall average reward $\\bar{R}$.\n",
    "\n",
    "For **precision@k** (we set k=32, same as episode length), here is how we are going to do it:\n",
    "1. Randomly select an user $i$ from 1 to num_users;\n",
    "2. Interact with this user (in the simulator) for certain steps (episode length e.g. 32) using current policy. \n",
    "   - During each step, look at the top-k (top-32) actions/items with highest probabilities. Then query the environment simulator to see how many of them have a reward of >= 0.2 (0.2 is the chosen threshould that corresponds 3 stars in our example). Then the proportion (e.g. 1/32 if only one of the 32 items with highest probabilities has an actual reward of >= 0.2) is the precision@k for this step. \n",
    "   - Repeat for 32 steps to compute 32 precision@k's and average over them to get the precision@k for this user.\n",
    "3. Go back to step 1 and repeat for multiple users, and average over all these precision@k to get the overall precision@k.\n",
    "\n",
    "For **recall@k** (k=32), this is how we are going to do it:\n",
    "1. Randomly select an user $i$ from 1 to num_users;\n",
    "2. Query the environment simulator for the reward about ALL the actions (from 1 to num_items) on this user $i$. Record those actions with a reward >= 0.2 (the same threshould as in precision@k); (minor note: If there is no such action (no relevant items), which I don't think will happen in this dataset, then the recall@k is set to 1 and done. No need to go to step 3.)\n",
    "3. Interact with this user (in the simulator) for certain steps (e.g. 32) using current policy.\n",
    "    - During each step, look at the top-k (top-32) actions/items with highest probabilities. Then query the environment simulator to see how many of them have a reward of >= 0.2 (0.2 is the chosen threshould that corresponds 3 stars in our example). Then the proportion (e.g. 1/134 if only one of the top-32 items with highest probabilities has an actual reward of >= 0.2 when 134 means this user $i$ has 134 relevant ratings) is the recall@k for this step.\n",
    "    - Repeat for 32 steps to compute 32 recall@k's and average over them to get the recall@k for this user.\n",
    "4. Go back to step 1 and repeat for multiple users, and average over all these recall@k to get the overall recall@k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_current_policy(threshould, num_users_to_interact, length_episode=32, k=32):\n",
    "    '''\n",
    "    params: threshould: threshould of reward/rating to distinguish relevant and recommended actions.\n",
    "                        Must be in normalized scale!!!\n",
    "            num_users_to_interact: number of users we will interact to average out the noiseness (an int >= 1)\n",
    "            length_episode: number of steps allowed for interaction for each user (an int >= 1)\n",
    "            k: k as used in precision@k, recall@k, and f1@k (an int >= 1)\n",
    "    \n",
    "    return: overall_average_reward (a float)\n",
    "            overall_precision_at_k (a float)\n",
    "            overall_recall_at_k (a folat)\n",
    "            overall_f1_at_k (a float)\n",
    "    '''\n",
    "    # check pre-conditions\n",
    "    assert 0 < threshould <= new_rating_range_high, 'Input threshould must be in normalized scale!!!'\n",
    "    assert num_users_to_interact >= 1\n",
    "    assert length_episode >= 1\n",
    "    assert k >= 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    overall_average_reward = 0.\n",
    "    overall_precision_at_k = 0.\n",
    "    overall_recall_at_k = 0.\n",
    "    overall_f1_at_k = 0.\n",
    "    \n",
    "    for idx in range(num_users_to_interact):\n",
    "        # Randomly select an user. Note that we index users from 1 to num_users!\n",
    "        user_id = np.random.randint(num_users) + 1\n",
    "        \n",
    "        # Average metrics over multiple interaction steps\n",
    "        average_reward = 0.\n",
    "        precision_at_k = 0.\n",
    "        recall_at_k = 0.\n",
    "        f1_at_k = 0.\n",
    "        \n",
    "        num_relevant_actions = 0\n",
    "        for action_id in range(1, num_items+1):\n",
    "            if get_rating(user_id, action_id) >= threshould:\n",
    "                num_relevant_actions += 1\n",
    "        if num_relevant_actions == 0:\n",
    "            recall_at_k = 1.\n",
    "            print('Oops, be careful because user No.' + str(user_id) + ' does not have any relevant ratings!')\n",
    "        \n",
    "        \n",
    "        ob = []\n",
    "        for t in range(length_episode):\n",
    "            # Compute average reward in an incremental way over multiple steps\n",
    "            action = sample_action_from_policy(ob)\n",
    "            reward = get_rating(user_id, action)\n",
    "            average_reward += 1/(t+1) * (reward - average_reward)\n",
    "            \n",
    "            \n",
    "            # Compute recommended actions (aka the k actions with highest probabilities)\n",
    "            probabilities = predict_policy_probabilities(ob)\n",
    "            # Pick out k actions/items with highest probabilities\n",
    "            actions_with_ascending_probability = np.argsort(probabilities)\n",
    "            recommended_actions = actions_with_ascending_probability[-k:] + 1 # remember that our actions are indexed from 1 to num_items!\n",
    "            \n",
    "            # number of recommended AND relevant actions, meaning how many actions (out the k actions)\n",
    "            # also have a true rating >= threshould\n",
    "            num_recommended_relevant_actions = 0 \n",
    "            for action_id in recommended_actions:\n",
    "                if get_rating(user_id, action_id) >= threshould:\n",
    "                    num_recommended_relevant_actions += 1\n",
    "                    \n",
    "            # Compute average precision@k in an incremental way over multiple steps\n",
    "            precision_at_k_this_step = num_recommended_relevant_actions / k\n",
    "            precision_at_k += 1/(t+1) * (precision_at_k_this_step - precision_at_k)\n",
    "            \n",
    "            # Compute average recall@k in an incremental way over multiple steps\n",
    "            recall_at_k_this_step = 1. # in case num_relevant_actions is 0\n",
    "            if num_relevant_actions != 0:\n",
    "                recall_at_k_this_step = num_recommended_relevant_actions / num_relevant_actions\n",
    "                recall_at_k += 1/(t+1) * (recall_at_k_this_step - recall_at_k)\n",
    "            \n",
    "            # Compute average f1@k in an incremental way over multiple steps\n",
    "            if (precision_at_k_this_step + recall_at_k_this_step) != 0.:\n",
    "                f1_at_k_this_step = 2 * precision_at_k_this_step * recall_at_k_this_step / (precision_at_k_this_step + recall_at_k_this_step)\n",
    "            else:\n",
    "                f1_at_k_this_step = 0.\n",
    "            f1_at_k += 1/(t+1) * (f1_at_k_this_step - f1_at_k)\n",
    "            \n",
    "            ob += [action, reward]\n",
    "        \n",
    "        \n",
    "        overall_average_reward += 1/(idx+1) * (average_reward - overall_average_reward)\n",
    "        overall_precision_at_k += 1/(idx+1) * (precision_at_k - overall_precision_at_k)\n",
    "        overall_recall_at_k += 1/(idx+1) * (recall_at_k - overall_recall_at_k)\n",
    "        overall_f1_at_k += 1/(idx+1) * (f1_at_k - overall_f1_at_k)\n",
    "        \n",
    "    return overall_average_reward, overall_precision_at_k, overall_recall_at_k, overall_f1_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Final Experiment with Evaluation\n",
    "\n",
    "Todo: Finish this cell (simply the documentation), add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(max_rounds, replay_buffer, buffer_ratio_retrain, length_episode, max_planning_steps=100):\n",
    "    '''\n",
    "    params: max_rounds: number of rounds for the training process -- one round stands for interacting and \n",
    "                        learning from multiple users and retrain user model once.\n",
    "            replay_buffer: a fixed-length queue with elements being interactive experience from \n",
    "                           various user with the environment. Or, simply use the return of the\n",
    "                           pre_train_user_model function. (a fixed-length deque);\n",
    "            buffer_ratio_retrain: periodically retrain our user model (aka reward prediction model) when this proportion\n",
    "                                  of replay buffer has been replaced with new interactive data (a float in (0,1])\n",
    "            length_episode: number of (real) steps allowed for each user for interaction in the dyna algorithm (an int >=1);\n",
    "            max_planning_steps: number of steps for planning for each user in the dyna algorithm (an int >=1).\n",
    "    '''\n",
    "    # check pre-conditions\n",
    "    # 1. make sure it is a valid replay buffer\n",
    "    if len(replay_buffer) == 0:\n",
    "        assert False, 'Please fill in replay buffer first!'\n",
    "    else:\n",
    "        for idx in range(1, len(replay_buffer)):\n",
    "            if len(replay_buffer[idx]) != len(replay_buffer[0]):\n",
    "                assert False, 'historical experience in replay_buffer must have same length!'\n",
    "    \n",
    "    # 2. make sure length_episode is equal to the length of existing element in replay_buffer\n",
    "    assert length_episode == (len(replay_buffer[0]) / 2), 'length_episode must have the same length as experiences in replay_buffer'\n",
    "    \n",
    "    \n",
    "    \n",
    "    periodically_retrain_steps = int(buffer_ratio_retrain * len(replay_buffer))\n",
    "\n",
    "    for idx in range(max_rounds):\n",
    "        print('\\n\\n' + str(idx) + 'th round of the main training process:')\n",
    "        \n",
    "        # Interact with multiple users (determined by periodically_retrain_steps)\n",
    "        # and learn from them in a model-based manner\n",
    "        for t in range(periodically_retrain_steps):\n",
    "            user_id = np.random.randint(num_users) + 1\n",
    "\n",
    "            ob = Dyna(user_id=user_id, max_iterations=length_episode, max_planning_steps=max_planning_steps)\n",
    "\n",
    "            replay_buffer.append(ob)\n",
    "            \n",
    "            if (t+1)%100 == 0 or t == 0:\n",
    "                print('    interaction with ' + str(t) + 'th user in this round finished')\n",
    "        \n",
    "        print('Retrain user model for the ' + str(idx) + 'th round.')\n",
    "        \n",
    "        # Periodically retrain user model from (updated) replay buffer\n",
    "        train_user_model_from_replay_buffer(replay_buffer)\n",
    "        print('Retraining user model is finished!')\n",
    "        \n",
    "        # Evaluate current policy\n",
    "        average_reward, precision_at_k, recall_at_k, f1_at_k = evaluate_current_policy(threshould=0.19, num_users_to_interact=len(replay_buffer), length_episode=length_episode, k=length_episode)\n",
    "        print('Performance of current policy:\\nAverage Reward = ' + str(average_reward) + ' precision@k = ' + str(precision_at_k) + ' recall@k = ' + str(recall_at_k) + ' f1@k = ' + str(f1_at_k) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check 4a (model-free solution) (NOT RECOMMENDED; READ THIS CELL CAREFULLY IF YOU INSIST!)\n",
    "Run experiments to get results!\n",
    "\n",
    "I am running the model-free algorithm again (with pre-train step)!\n",
    "\n",
    "I will save the pre-trained user model so that later, when I run model-based algorithm (remember to freeze the RNN weights for periodically retraining to stick to the same state representation. Now  I did not do that and it does not matter because we will not retrain the user model anyway), I will load this weights so 1. I don't need to pretrain again and 2. I have the same initial state representation.\n",
    "\n",
    "Currently, the code is for the model-based solution. Just run each cell one by one (except for those sanity check ones and this one) to 4b to perform the model-based algorithm. \n",
    "If you wish to run the model-free algorithm (not recommended), do the folloing three modifications to the code:\n",
    "1. comment out the freeze-weights codes in section \"reward prediction model\" and uncomment a same code block in section \"state-value predictio network\";\n",
    "2. comment out the planning module in Dyna function;\n",
    "3. comment out the periodically retrain statement (line 47) in the main_train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_size = 3000\n",
    "length_episode = 32\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Pre-train the user model (state representation model + reward prediction model)\n",
    "replay_buffer = pre_train_user_model(num_users_to_interact=replay_buffer_size, length_episode=length_episode)\n",
    "\n",
    "print('Time to pre-train the user model is ' + str((time.time() - start_time) / 60) + ' minutes')\n",
    "print('Save the weights of the user model to weights_pretrained_user_model.h5')\n",
    "reward_prediction_model.save_weights('weights_pretrained_user_model.h5')\n",
    "\n",
    "\n",
    "main_train(max_rounds=60, replay_buffer=replay_buffer, buffer_ratio_retrain=1.0, length_episode=32, max_planning_steps=5)\n",
    "\n",
    "print('\\n\\nTotal time to run the program is ' + str((time.time() - start_time) / 60) + ' minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check 4b (model-based solution)\n",
    "Having run the previous cell of code for the model-free solution, we are now ready to try out our model-based solution!\n",
    "\n",
    "We need to modify the code in the following way:\n",
    "1. comment out the freeze-weights codes in section \"state-value predictio network\" and move it to reward prediction model. We do this because, in the model-free solution, we have pre-trained the user model and saved the weights of the user model after pre-training step. In our model-based solution, for fair comparison, we want to start our user model with the same weight AND during later periodical re-training of the user model, we do not want to change the weights of the state representation model but only the reward prediction model. Thus, we will omit the pre-training step in model-based solution and only load the saved weights for our user model. In later periodical retraining of the user model, we will only update the reward prediction model since we have  frozen the weights of the state representation model at the very begining.\n",
    "\n",
    "2. uncomment the planning module in dyna.\n",
    "\n",
    "3. uncomment the periodically retain step in main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 0th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.00574791666666667 precision@k = 0.246866536458333 recall@k = 0.09354745358453632 f1@k = 0.11449064686328614\n",
      "\n",
      "\n",
      "\n",
      "1th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 1th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.31284 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.006100000000000004 precision@k = 0.27201464843749956 recall@k = 0.11421334863178689 f1@k = 0.13586534279481938\n",
      "\n",
      "\n",
      "\n",
      "2th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 2th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.23862 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005833333333333338 precision@k = 0.2936832682291661 recall@k = 0.1205737393508476 f1@k = 0.14317754848518038\n",
      "\n",
      "\n",
      "\n",
      "3th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 3th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005535416666666663 precision@k = 0.3035553385416665 recall@k = 0.1280746660918267 f1@k = 0.15150393315540828\n",
      "\n",
      "\n",
      "\n",
      "4th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 4th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005685416666666677 precision@k = 0.30439550781250085 recall@k = 0.12229813585686863 f1@k = 0.14841614159264183\n",
      "\n",
      "\n",
      "\n",
      "5th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 5th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.47033 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005379166666666658 precision@k = 0.2984417317708329 recall@k = 0.12543594162602234 f1@k = 0.14937197055431056\n",
      "\n",
      "\n",
      "\n",
      "6th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 6th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.46226 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005616666666666678 precision@k = 0.30374153645833346 recall@k = 0.12752550067994017 f1@k = 0.15191899193960975\n",
      "\n",
      "\n",
      "\n",
      "7th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 7th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.0058854166666666725 precision@k = 0.3101399739583334 recall@k = 0.12864155557237936 f1@k = 0.1540310500215742\n",
      "\n",
      "\n",
      "\n",
      "8th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 8th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005689583333333341 precision@k = 0.3142366536458329 recall@k = 0.12919053352939294 f1@k = 0.15485193528901392\n",
      "\n",
      "\n",
      "\n",
      "9th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 9th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.27696 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005337500000000008 precision@k = 0.3026757812500004 recall@k = 0.13024013945889906 f1@k = 0.15402091396744888\n",
      "\n",
      "\n",
      "\n",
      "10th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 10th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005633333333333335 precision@k = 0.3156263020833329 recall@k = 0.12934722055462045 f1@k = 0.15469771156546977\n",
      "\n",
      "\n",
      "\n",
      "11th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 11th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.29821 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.0059145833333333385 precision@k = 0.31815950520833197 recall@k = 0.13566374736629644 f1@k = 0.16043603832247932\n",
      "\n",
      "\n",
      "\n",
      "12th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 12th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005727083333333324 precision@k = 0.3221373697916669 recall@k = 0.13424103277478 f1@k = 0.160435348435308\n",
      "\n",
      "\n",
      "\n",
      "13th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 13th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.0061375 precision@k = 0.3228854166666661 recall@k = 0.13341264985474297 f1@k = 0.15894446798114\n",
      "\n",
      "\n",
      "\n",
      "14th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 14th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005845833333333341 precision@k = 0.32310384114583257 recall@k = 0.1362400680389614 f1@k = 0.1614735728084914\n",
      "\n",
      "\n",
      "\n",
      "15th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 15th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.23553 does not have any relevant ratings!\n",
      "Oops, be careful because user No.23862 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.00589166666666667 precision@k = 0.3235104166666666 recall@k = 0.13446548535694322 f1@k = 0.16002811345460327\n",
      "\n",
      "\n",
      "\n",
      "16th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 16th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.31284 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.00630625000000001 precision@k = 0.3213398437500003 recall@k = 0.13555252748108618 f1@k = 0.1607850202780067\n",
      "\n",
      "\n",
      "\n",
      "17th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 17th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.006487500000000002 precision@k = 0.32783203125000066 recall@k = 0.135021006539427 f1@k = 0.16231450466619543\n",
      "\n",
      "\n",
      "\n",
      "18th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 18th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.29965 does not have any relevant ratings!\n",
      "Oops, be careful because user No.23553 does not have any relevant ratings!\n",
      "Oops, be careful because user No.61436 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.0068125000000000034 precision@k = 0.3198541666666667 recall@k = 0.13194021976380227 f1@k = 0.15699227024411735\n",
      "\n",
      "\n",
      "\n",
      "19th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 19th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.005985416666666682 precision@k = 0.3169833984375006 recall@k = 0.1355928243854808 f1@k = 0.1607268273015319\n",
      "\n",
      "\n",
      "\n",
      "20th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 20th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.42597 does not have any relevant ratings!\n",
      "Oops, be careful because user No.47033 does not have any relevant ratings!\n",
      "Oops, be careful because user No.42597 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.007049999999999991 precision@k = 0.331241536458334 recall@k = 0.13918502792075688 f1@k = 0.164213169847846\n",
      "\n",
      "\n",
      "\n",
      "21th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 21th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.0064312500000000055 precision@k = 0.32483333333333336 recall@k = 0.13915432296608934 f1@k = 0.16484831009646245\n",
      "\n",
      "\n",
      "\n",
      "22th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 22th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.006191666666666673 precision@k = 0.3227708333333337 recall@k = 0.13449751274364757 f1@k = 0.16131725163435356\n",
      "\n",
      "\n",
      "\n",
      "23th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 23th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.006568749999999996 precision@k = 0.32272102864583385 recall@k = 0.13126142036254082 f1@k = 0.15827899722559344\n",
      "\n",
      "\n",
      "\n",
      "24th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 24th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.47033 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.007158333333333329 precision@k = 0.3306412760416665 recall@k = 0.13535219762516526 f1@k = 0.16168943014237863\n",
      "\n",
      "\n",
      "\n",
      "25th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 25th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.61992 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.006970833333333331 precision@k = 0.3308486328124997 recall@k = 0.13852840083621393 f1@k = 0.16517701294379838\n",
      "\n",
      "\n",
      "\n",
      "26th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 26th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.46226 does not have any relevant ratings!\n",
      "Oops, be careful because user No.61436 does not have any relevant ratings!\n",
      "Oops, be careful because user No.23862 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.007518750000000008 precision@k = 0.33146940104166633 recall@k = 0.1358321744429358 f1@k = 0.16216198126368078\n",
      "\n",
      "\n",
      "\n",
      "27th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 27th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.29965 does not have any relevant ratings!\n",
      "Oops, be careful because user No.31284 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.0070958333333333195 precision@k = 0.3313808593749996 recall@k = 0.13653542493316054 f1@k = 0.16324622963347915\n",
      "\n",
      "\n",
      "\n",
      "28th round of the main training process:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 28th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.46226 does not have any relevant ratings!\n",
      "Oops, be careful because user No.48722 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.007447916666666654 precision@k = 0.32139192708333375 recall@k = 0.13453948039409544 f1@k = 0.15882128180959712\n",
      "\n",
      "\n",
      "\n",
      "29th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 29th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.61436 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.007889583333333339 precision@k = 0.3270885416666664 recall@k = 0.1310886632077882 f1@k = 0.15803441410379152\n",
      "\n",
      "\n",
      "\n",
      "30th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 30th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.008139583333333346 precision@k = 0.3325485026041668 recall@k = 0.13504094879781328 f1@k = 0.1628002673201678\n",
      "\n",
      "\n",
      "\n",
      "31th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 31th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.010585416666666668 precision@k = 0.3224378255208333 recall@k = 0.13240774104948375 f1@k = 0.1592986819598029\n",
      "\n",
      "\n",
      "\n",
      "32th round of the main training process:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 32th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.32956874999999963 precision@k = 0.32828125 recall@k = 0.13102574073706633 f1@k = 0.15749205307131822\n",
      "\n",
      "\n",
      "\n",
      "33th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 33th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3463374999999994 precision@k = 0.32668749999999996 recall@k = 0.13460421540756423 f1@k = 0.16125306474216014\n",
      "\n",
      "\n",
      "\n",
      "34th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 34th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.29965 does not have any relevant ratings!\n",
      "Oops, be careful because user No.48079 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3345979166666674 precision@k = 0.3206188151041654 recall@k = 0.12984760590556996 f1@k = 0.1552011393780465\n",
      "\n",
      "\n",
      "\n",
      "35th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 35th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3317895833333337 precision@k = 0.29928124999999983 recall@k = 0.11952583758375947 f1@k = 0.14474862339630526\n",
      "\n",
      "\n",
      "\n",
      "36th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 36th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.42597 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3376 precision@k = 0.29986458333333366 recall@k = 0.1257890541094793 f1@k = 0.14913946389029553\n",
      "\n",
      "\n",
      "\n",
      "37th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 37th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.31284 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3349770833333328 precision@k = 0.3045208333333339 recall@k = 0.12120031925211949 f1@k = 0.14597380163595647\n",
      "\n",
      "\n",
      "\n",
      "38th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 38th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3205145833333332 precision@k = 0.2939791666666669 recall@k = 0.1222495803190118 f1@k = 0.1455329535826109\n",
      "\n",
      "\n",
      "\n",
      "39th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 39th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.33834166666666676 precision@k = 0.2995624999999998 recall@k = 0.12390664969263289 f1@k = 0.14835837449673273\n",
      "\n",
      "\n",
      "\n",
      "40th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 40th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3192958333333332 precision@k = 0.2948333333333327 recall@k = 0.12068776187853354 f1@k = 0.14510568864125176\n",
      "\n",
      "\n",
      "\n",
      "41th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 41th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.33291875000000054 precision@k = 0.29242708333333256 recall@k = 0.1241931600977859 f1@k = 0.1484704584034885\n",
      "\n",
      "\n",
      "\n",
      "42th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 42th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.32569999999999993 precision@k = 0.29593749999999985 recall@k = 0.12249507060712854 f1@k = 0.1468592805326857\n",
      "\n",
      "\n",
      "\n",
      "43th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 43th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3333187500000001 precision@k = 0.3029270833333338 recall@k = 0.12014859884817544 f1@k = 0.14501120956060093\n",
      "\n",
      "\n",
      "\n",
      "44th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 44th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3249312500000006 precision@k = 0.2955937499999997 recall@k = 0.12145819484672832 f1@k = 0.14491485864912343\n",
      "\n",
      "\n",
      "\n",
      "45th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 45th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.27696 does not have any relevant ratings!\n",
      "Oops, be careful because user No.3325 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.32252291666666677 precision@k = 0.2931250000000001 recall@k = 0.12007812570456705 f1@k = 0.14328006181277872\n",
      "\n",
      "\n",
      "\n",
      "46th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 46th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.27696 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.32499583333333343 precision@k = 0.29377083333333276 recall@k = 0.1194885329855701 f1@k = 0.1434874352683717\n",
      "\n",
      "\n",
      "\n",
      "47th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 47th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3333229166666669 precision@k = 0.2972083333333342 recall@k = 0.1215871442327001 f1@k = 0.14566318353128038\n",
      "\n",
      "\n",
      "\n",
      "48th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 48th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.32545208333333314 precision@k = 0.29267708333333337 recall@k = 0.12195869198571598 f1@k = 0.14624346523611145\n",
      "\n",
      "\n",
      "\n",
      "49th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 49th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3295187499999996 precision@k = 0.297 recall@k = 0.1210976316507358 f1@k = 0.14485441648900438\n",
      "\n",
      "\n",
      "\n",
      "50th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 50th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.3325 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3188666666666661 precision@k = 0.2977604166666663 recall@k = 0.12016869312931036 f1@k = 0.1450485634851628\n",
      "\n",
      "\n",
      "\n",
      "51th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 51th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.48722 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.33726666666666666 precision@k = 0.2908333333333333 recall@k = 0.12113273272886213 f1@k = 0.1440468659807431\n",
      "\n",
      "\n",
      "\n",
      "52th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 52th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.48722 does not have any relevant ratings!\n",
      "Oops, be careful because user No.61992 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.31344791666666694 precision@k = 0.2897500000000004 recall@k = 0.11952281086049069 f1@k = 0.14272009952242581\n",
      "\n",
      "\n",
      "\n",
      "53th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 53th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.33344583333333316 precision@k = 0.30381249999999976 recall@k = 0.11990194832490551 f1@k = 0.14455991836093243\n",
      "\n",
      "\n",
      "\n",
      "54th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 54th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.32938333333333375 precision@k = 0.29954166666666654 recall@k = 0.12102000126264224 f1@k = 0.1461565810063335\n",
      "\n",
      "\n",
      "\n",
      "55th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 55th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3193229166666667 precision@k = 0.2924479166666668 recall@k = 0.12114555797580612 f1@k = 0.1450306029091333\n",
      "\n",
      "\n",
      "\n",
      "56th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 56th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.32825833333333326 precision@k = 0.29556250000000023 recall@k = 0.11975834752135424 f1@k = 0.14441424744601372\n",
      "\n",
      "\n",
      "\n",
      "57th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 57th round.\n",
      "Retraining user model is finished!\n",
      "Oops, be careful because user No.27696 does not have any relevant ratings!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.32346041666666664 precision@k = 0.29664583333333416 recall@k = 0.12223587195834792 f1@k = 0.14634546850967114\n",
      "\n",
      "\n",
      "\n",
      "58th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 58th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.33033333333333326 precision@k = 0.2954687500000002 recall@k = 0.11961876618026694 f1@k = 0.14364020519108303\n",
      "\n",
      "\n",
      "\n",
      "59th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n",
      "    interaction with 1899th user in this round finished\n",
      "    interaction with 1999th user in this round finished\n",
      "    interaction with 2099th user in this round finished\n",
      "    interaction with 2199th user in this round finished\n",
      "    interaction with 2299th user in this round finished\n",
      "    interaction with 2399th user in this round finished\n",
      "    interaction with 2499th user in this round finished\n",
      "    interaction with 2599th user in this round finished\n",
      "    interaction with 2699th user in this round finished\n",
      "    interaction with 2799th user in this round finished\n",
      "    interaction with 2899th user in this round finished\n",
      "    interaction with 2999th user in this round finished\n",
      "Retrain user model for the 59th round.\n",
      "Retraining user model is finished!\n",
      "Performance of current policy:\n",
      "Average Reward = 0.3388541666666663 precision@k = 0.29762500000000036 recall@k = 0.12139062490561274 f1@k = 0.1462259531923419\n",
      "\n",
      "\n",
      "\n",
      "Total time to run the program is 4895.448354987303 minutes\n"
     ]
    }
   ],
   "source": [
    "replay_buffer_size = 3000\n",
    "length_episode = 32\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load (saved weights) for the user model to ensure same \n",
    "# initialization of the state representation.\n",
    "reward_prediction_model.load_weights('weights_pretrained_user_model.h5')\n",
    "\n",
    "# We don't have pre-train here to return a full replay buffer for us,\n",
    "# which is required in the main function. We will just create one and stuff \n",
    "# it with replay_buffer_size meaningless lists. Those replay_buffer_size lists\n",
    "# will be replaced by new interactive data in the first round before re-training\n",
    "# the user model on the replay buffer, so this is okay!\n",
    "replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "for _ in range(replay_buffer_size):\n",
    "    fake_ob = []\n",
    "    for _ in range(length_episode):\n",
    "        fake_ob += [0,0]\n",
    "    replay_buffer.append(fake_ob)\n",
    "\n",
    "main_train(max_rounds=60, replay_buffer=replay_buffer, buffer_ratio_retrain=1.0, length_episode=32, max_planning_steps=5)\n",
    "\n",
    "print('\\n\\nTotal time to run the program is ' + str((time.time() - start_time) / 60) + ' minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining Issues:\n",
    "\n",
    "~~`Question:`~~ When updating the policy network parameters, do we update the state representation model as well?\n",
    "\n",
    "~~`Question:`~~ Similarly, when we update the state-value prediction network, do we update the state representation model?\n",
    "\n",
    "**Answer:** it is unknown. Maybe try both! Currently we don't.\n",
    "    \n",
    "~~`Question:`~~ What about the discounting factor $\\gamma$? Do we use it in our update?\n",
    "\n",
    "**Answer:** Don't use it at this moment. Maybe Try it if I have time! \n",
    "    \n",
    "`Question:` When t goes up, the user's historical records increases (ob becomes a long long list), which affects the speed of computing many functions needed to update the policy. Is it reasonable for us to keep track of only the most recent K records as our observation?\n",
    "\n",
    "**Answer:** No, since the observation sequence won't get too long.    \n",
    "\n",
    "~~`Question:`~~ When we fill in the replay buffer with interactive experience with the user, a lot of (simulated) rewards are 0 not because they're actually 0 but rather because they are unknown and we artificially set it to 0. Then when we use the replay buffer to train our user model (the reward prediction model), we are forcing the model to learn that the rewards for those actions are 0's, which might not be the case. We can try to use only experiences with real ratings to train the user model, but I am now not sure how to do it. Anyway, let's just do the former solution first and see what happens.\n",
    "\n",
    "~~`Question:`~~ Parameter tuning\n",
    "\n",
    "~~`Question:`~~ case study to understand its behavior\n",
    "\n",
    "`Question:` User model overfitting (poor generalization?)\n",
    "\n",
    "~~`Question:`~~ Try multi-step methods or eligibility trace for the policy-gradient method.\n",
    "\n",
    "~~`Question:`~~ What about a model-free solution but with experience replay?\n",
    "\n",
    "`Question:` $R_0$ might should be 2.5 rather than 0, because after normalization, $R_0$ becomes -1.\n",
    "\n",
    "~~`Question:`~~ Are precision@n, recall@n, f1@n really reasonable evaluation metrics?\n",
    "\n",
    "**Answer:** precision@n: what if some user has just a few (say < n) relevant items, then precision@n for this user will never be able to get close to 1 no matter how good the recommendation policy is.\n",
    "\n",
    "recall@n: what if some user has NO relevant items (denominator is 0), should we set the recall@n for this user to 1, as we do in conventional recommendation systems? Also, the denominator is related to number of relevant items of the user, rather than related to n, so the range of recall@n is no longer [0,1] depending on the quality of the recommendation policy. Thus, I personally think it is no longer suitable metrics for our problem.\n",
    "\n",
    "f1@n: if recall@n does not make sense in the first place, how can f1@n, one quantity that relies on precision@n and recall@n, make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
