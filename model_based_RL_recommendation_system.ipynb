{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. User Model \n",
    "In the first part of this notebook, we built up the user model part for our model-based approach to the recommendation problem. The user model consists of two parts -- a state representation model and a reward prediction model. The goal is to train such a model that takes as input a user's historical record $(a_1,r_1, ..., a_{t-1},r_{t-1},a_t)$ and output/predict what the user's feedback r_t will be. Formally, try to train a model $\\hat{r}$ parameterized by $\\theta$ such that $\\hat{r}_\\theta(o_{t-1},a_t) \\approx r(o_{t-1}, a_t)$.\n",
    "\n",
    "### State Representation Model\n",
    "We used a RNN layer to encode historically records of an user, $(a_1,r_1, a_2,r_2,..., a_{t-1},r_{t-1})$, into a fixed-length vector which we call $S_t$. Notice that the size of the input of the RNN (user's historical records) gradually increases over time so we decided to use a RNN to convert it into a fixed-length vector.\n",
    "\n",
    "### Reward Prediction Model\n",
    "After getting our state representation of the user's history, we concatenate it with the user's latest action a_t and feed it into a simple fully-connected layer to output our prediction of $\\hat{r}_t$.\n",
    "\n",
    "### Item/Action Embedding\n",
    "One extra thing we did is to embed each item (or action) in the store into a vector in some high-dimensional space. For example, say we have in total 10,000 items in the repository, and we index each item with a number between 1 and 10,000. When we have a user's historical records and want to feed into the model, we need to feed all those actions $(a_1, a_2, ..., a_{t-1}, a_t)$ into a embedding layer first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to train the user model in a supervised manner with all users' historical records, what we are going to do first is to implement how to do this with *one* user's historical records -- $(a_1, r_1, ..., a_t, r_t)$. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONHASHSEED=0\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Flatten, SimpleRNN, Concatenate, Lambda, Softmax\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from collections import deque\n",
    "from scipy.sparse import coo_matrix\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import random as rn\n",
    "\n",
    "# For strict reproducible purpose\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1)\n",
    "tf.set_random_seed(1234)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Preprocess the raw input data from some user $(a_1, r_1, ..., a_t, r_t)$ into a bunch of input-output pairs where input is $(a_1,r_1,...,a_i)$ and output is $r_i$ for all $i$ in $[1,t]$.\n",
    "\n",
    "- We will first slice it into different input-output pairs:\n",
    "    - $a_1 \\rightarrow r_1$\n",
    "    - $a_1, r_1, a_2 \\rightarrow r_2$\n",
    "    - ...\n",
    "    - $a_1, r_1, ..., a_t \\rightarrow r_t$\n",
    "\n",
    "(The next two steps are intended for matching up the input/output shape of the Keras model we are going to build next. Feel free to skip it now :))\n",
    "- Secondly, for each input-output pair, say, $a_1, r_1, ..., a_7, r_7, a_8 \\rightarrow r_8$, I need to extract two inputs and one output from it which will later be fed into the model:\n",
    "    - input1 = $0, a_1, a_2, ..., a_7, a_8$\n",
    "    - input2 = $0, r_1, r_2, ..., r_7$\n",
    "    - output = $r_8$\n",
    "\n",
    "- Finally, convert all those lists (input1, input2, output) into numpy arrays and reshape them to be the correct shape for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the user's observation into strucutred format\n",
    "# that can later be fed into the reward prediction model\n",
    "# to either predict reward or get state representation\n",
    "# (NOTE: this function works fine even if observation is an empty list. This happens\n",
    "#  when we want to get the initial state represenation of the MDP.)\n",
    "def obs2modelinputs(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}] (for state representation purpose)\n",
    "                         or [a_1,r_1,...,a_{t-1},r_{t-1},a_t] (for reward prediction purpose)\n",
    "    \n",
    "    return: observation's corresponding structured data that can be directly fed into the reward prediction model\n",
    "    '''\n",
    "    \n",
    "    if len(observation) % 2 == 0:\n",
    "        # To get state representation, we simply append a 0 to observation as a 'fake' a_t\n",
    "        # This is okay since we won't use a_t anyway when computing the state representation of observation\n",
    "        action_inputs = np.array([0] + observation[::2] + [0]) \n",
    "    else:\n",
    "        action_inputs = np.array([0] + observation[::2])\n",
    "    \n",
    "    reward_inputs = np.array([0] + observation[1::2])\n",
    "    \n",
    "    # Reshape inputs so that the reward prediction model\n",
    "    # or the state representation model can accpet\n",
    "    action_inputs = action_inputs.reshape(1,-1)\n",
    "    reward_inputs = reward_inputs.reshape(1,-1)\n",
    "    \n",
    "    return action_inputs, reward_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct training set -- a list of training samples obs_{t-1},a_t --> r_t, t = {1,2,...T}\n",
    "# in strucutured format -- from one user's current historical records [a_1,r_1,...,a_T,r_T]\n",
    "# The return can directly be sampled one at a time and feed into the reward prediction for training\n",
    "def construct_strucutred_training_set(observation):\n",
    "    '''\n",
    "    param:\n",
    "    observation: a list of this user's interactions with the recommendation system\n",
    "                 [a_1,r_1,...,a_T,r_T]\n",
    "    \n",
    "    return:\n",
    "    structured_training_set: a list of training samples where each element in this list is\n",
    "    a tuple (inputs, outputs) for that training sample.\n",
    "    '''\n",
    "    \n",
    "    assert len(observation)%2 == 0, 'Please make sure the observation on which \\\n",
    "                                     you want to construct training set has even number of elements.'\n",
    "    \n",
    "    T = len(observation) // 2\n",
    "    # training_set stores all the training samples sliced from the observation\n",
    "    # each training sample in it is a tuple ([a_1,r_1,...,a_t], r_t) for t = {1,2,...T}\n",
    "    training_set = [] \n",
    "    for t in range(T):\n",
    "        x = observation[0 : 2 * t + 1] # a list   [a_1,r_1,...a_t]\n",
    "        y = observation[2 * t + 1]     # a scalar r_t\n",
    "        training_set.append((x, y))\n",
    "    \n",
    "    # Convert each training sample in training_set into structured format\n",
    "    structured_training_set = []\n",
    "    for sample in training_set:\n",
    "        x = sample[0] # [a_1,r_1,...,a_t]\n",
    "        y = sample[1] # r_t\n",
    "        \n",
    "        action_inputs, reward_inputs = obs2modelinputs(x)\n",
    "        # Do a similar thing to output/r_t\n",
    "        reward_output = np.array(y).reshape(1,1)\n",
    "        \n",
    "        structured_training_set.append(([action_inputs, reward_inputs], [reward_output]))\n",
    "    \n",
    "    return structured_training_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator function \n",
    "it generats one batch of training samples at a time for the *reward prediction model* to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_sample_generator(structured_training_set):\n",
    "    '''\n",
    "    params: \n",
    "    structured_data: Return of the construct_strucutred_training_set function. \n",
    "    '''\n",
    "    while True:\n",
    "        # Randomly yield one training sample from the training set without repetition\n",
    "        # before consuming all training samples and starting a new round\n",
    "        np.random.shuffle(structured_training_set)\n",
    "        for training_sample in structured_training_set:\n",
    "            yield training_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building-up layers for the user model\n",
    "We will define the necessary layers required for our user model. \n",
    "Later, we will build two models, reward prediction model and policy model, from those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 69878 # total number of users -- used in the environment simulator\n",
    "num_items = 10677 # total number of items -- cardinality of action space |A|\n",
    "dimension_action_embedding = 10\n",
    "dimension_state_representation = 20\n",
    "gamma = 0.99 # Discounting factor\n",
    "\n",
    "r_low = 0\n",
    "r_high = 5\n",
    "dimension_reward_ohe = 10\n",
    "\n",
    "# Three functions that will later be used to construct Lambda layers\n",
    "def tminusone_actions(action_embeddings):\n",
    "    # action_embeddings.shape = (batch_size, time_steps, dimension_action_embedding)\n",
    "    return action_embeddings[:,:-1,:]\n",
    "\n",
    "def action_t(action_embeddings):\n",
    "    # action_embeddings.shape = (batch_size, time_steps, dimension_action_embedding) \n",
    "    return action_embeddings[:,-1,:]\n",
    "\n",
    "def one_hot(reward_inputs):\n",
    "    # reward_inputs.shape = (batch_size, time_steps)\n",
    "    # We implicitly use broad-casting here since dimension_reward_ohe, r_low, r_high are all scalars\n",
    "    indices = dimension_reward_ohe - (dimension_reward_ohe * (r_high - reward_inputs)) // (r_high - r_low)\n",
    "    \n",
    "    # indices is a rank-2 tensor whose datat type is floating point although each\n",
    "    # element in indices is already integer (e.g. 3.0, 8.0, 9.0, etc.). We thus cast\n",
    "    # it to integer data type so that one_hot function call be evoked correctly\n",
    "    indices = tf.dtypes.cast(indices, 'int32')\n",
    "    \n",
    "    # shape of return = (batch_size, time_steps, dimension_reward_ohe)\n",
    "    return K.one_hot(indices, dimension_reward_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Input Layers (notice the slightly asymmetry b/t these two inputs)\n",
    "action_inputs = Input(shape=(None,), name='action_inputs', dtype='int32') # 0,a_1,a_2,...,a_{t-1},a_t. 'None' here means any t can be expected\n",
    "reward_inputs = Input(shape=(None,), name='reward_inputs', dtype='float32') # 0,r_1,r_2,...,r_{t-1}.     'None' here means any t can be expexted\n",
    "\n",
    "\n",
    "# Embedding layer for actions\n",
    "# input_dim = num_items + 1 because items are ranked from 1 to num_items. We reserve 0 for the dummy action\n",
    "action_embeddings = Embedding(input_dim=num_items+1, output_dim=dimension_action_embedding, name='action_embeddings')(action_inputs)\n",
    "\n",
    "# Split the output to two parts -- embeddings for (0,a_1,...,a_{t-1}) and embedding for a_t\n",
    "tminusone_action_embeddings = Lambda(tminusone_actions)(action_embeddings)\n",
    "action_t_embedding = Lambda(action_t)(action_embeddings)\n",
    "\n",
    "# One-hot encoding reward inputs\n",
    "t_minusone_reward_ohes = Lambda(one_hot)(reward_inputs)\n",
    "\n",
    "# RNN layer for state representation\n",
    "rnn_inputs = Concatenate(axis=-1)([tminusone_action_embeddings, t_minusone_reward_ohes])\n",
    "state_representation = SimpleRNN(dimension_state_representation, name='state_representation')(rnn_inputs)\n",
    "\n",
    "\n",
    "# A dense layer for reward prediction\n",
    "reward_prediction_inputs = Concatenate(axis=-1)([state_representation, action_t_embedding])\n",
    "reward_prediction = Dense(1)(reward_prediction_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward Prediction Model\n",
    "The reward prediction model consist of an action/item embedding layer + a RNN (aka state representation layer) + a reward prediction model (fully-connected layer).\n",
    "\n",
    "The inputs of training data for this model are of various length and we have only one training sample for each length. Thus, we feed each training sample to the model and train the network on each sample (like stochastic gradient descent). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward prediction model\n",
    "reward_prediction_model = Model(inputs=[action_inputs, reward_inputs], outputs=reward_prediction)\n",
    "\n",
    "reward_prediction_model.get_layer('action_embeddings').trainable = False\n",
    "reward_prediction_model.get_layer('state_representation').trainable = False\n",
    "\n",
    "# Model compilation: 'rmsprop' is recommended for rnn and 'mse' is used \n",
    "# because we have a regression problem\n",
    "reward_prediction_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "# Train the reward prediction model on user's observation \n",
    "def train_reward_prediction_model(observation, steps_per_epoch=1000, epochs=10):\n",
    "    '''\n",
    "    params: observation: a list of user's current historical records [a_1,r_1,...,a_t,r_t]\n",
    "            steps_per_epoch: steps to train in each epoch\n",
    "            epochs: number of total training epochs \n",
    "    '''\n",
    "\n",
    "    structured_training_set = construct_strucutred_training_set(observation)\n",
    "    reward_prediction_model.fit_generator(training_sample_generator(structured_training_set), steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=0)\n",
    "\n",
    "# Predict the user's feedback/reward for action based on his/her historical records (observation)\n",
    "def predict_reward(observation, action):\n",
    "    '''\n",
    "    params: observation: a list of user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "            action: the action just taken a_t (a scalar)\n",
    "            \n",
    "    return: predicted reward for the item just recommended \\hat{r}_t (a scalar)\n",
    "    '''\n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation + [action])\n",
    "    return reward_prediction_model.predict([action_inputs, reward_inputs])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Representation Model\n",
    "It turns out that we are going to need to compute the current state $S_t$ given the user's current historical record $O_{t-1}$. Thus, let's define a state representation model which we can conveniently use to encode history into states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well, this state representation model is not necessary.\n",
    "# We still define such a model for debugging convenience\n",
    "# since we might want to take a look at what the state \n",
    "# representation is like.\n",
    "state_representation_model = Model(inputs=reward_prediction_model.input, outputs=reward_prediction_model.get_layer(name='state_representation').output)\n",
    "\n",
    "def get_state_representation(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's historica records to be encoded [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "    \n",
    "    return: an encoded vector representing S_t. Shape = (1, dimension_state_representation).\n",
    "    '''\n",
    "    assert len(observation) % 2 == 0, 'historical records to be encoded to a state must have even length.'\n",
    "    \n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    return state_representation_model.predict([action_inputs, reward_inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-step Actor-Critic to Improve our recommendation policy\n",
    "Now we are going to build up the policy network. After receiving each transition $(S_t,A_t,R_t,S_{t+1})$, we can use Actor-Critic algorithm (one-step) to update our policy network as well as our state-value prediction network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose the *One-step Actor-Critic* algorithm in RL to update the policy network. Namely, the algorithm requires the policy to chose an action $A_t$ in a given state $S_t$ (computed from $O_{t-1}$). Take this action and observe reward $R_t$ and next state $S_{t+1}$ (computed from $O_t$). Next we compute the temporal-difference error $\\delta=R_t + \\gamma*\\hat{v}(S_{t+1};w) - \\hat{v}(S_t;w)$, where $\\hat{v}(S;w)$ is a network (function approximator) parameterized by $w$ that tries to predict state-values. With this TD error, we can update the state-value prediction network and later the policy network. The details of how to update those two networks are discussed later. Now let's first try to build up a state-value prediction network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-value Prediction Network\n",
    "We implement a simple fully connected layer to predict the state-value of a given state encoded from observation: $\\hat{v}(O_{t-1};w) \\approx v(S_t)$.\n",
    "\n",
    "#### Training the State-value Prediction Network with One-Step Transition\n",
    "After receiving the one-step transition $(S_t,A_t,R_t,S_{t+1})$, we train/update the state-value prediction network as follows:\n",
    "- first compute the temporal-difference error $\\delta = R_t + \\gamma*\\hat{v}(S_{t+1};w)$;\n",
    "- secondly, fit the model with (one) training sample $(O_{t-1}, \\delta)$ using 'mean_squared_error' loss. Furthermore, I choose 'sgd' optimizer since it is simple and we will be training this state-value prediction network with only one transition tuple anyway, meaning batch_size=1, so it does not make sense to use advanced optimizer which will only have an effect when there are more than more training sample in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up fully connected layers (here it is just one layer) \n",
    "# after the state representation network to predict state-values\n",
    "# of the state representation\n",
    "state_value_prediction = Dense(1)(state_representation)\n",
    "\n",
    "state_value_prediction_model = Model(inputs=[action_inputs, reward_inputs], outputs=state_value_prediction)\n",
    "state_value_prediction_model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "\n",
    "def predict_state_value(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's historica records to be encoded [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "    \n",
    "    return: predicted state value of the encoded state from observation (a scalar)\n",
    "    '''\n",
    "    assert len(observation) % 2 == 0, 'historical records to be encoded to a state must have even length.'\n",
    "    \n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    return state_value_prediction_model.predict([action_inputs, reward_inputs])[0][0]\n",
    "    \n",
    "    \n",
    "def train_state_value_prediction_model_with_one_step_transition(observation, action, reward):\n",
    "    '''\n",
    "    params: observation: the user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "            action: the action a_t just taken by the recommendation system (a scalar)\n",
    "            reward: user's feedback on the action/item just taken (a scalar)\n",
    "    '''\n",
    "    TD_error = reward + gamma * predict_state_value(observation + [action, reward])\n",
    "    \n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    # Reshape TD_error so that it can be fed to model.fit function\n",
    "    TD_error = np.array(TD_error).reshape(1,-1)\n",
    "    \n",
    "    state_value_prediction_model.fit([action_inputs,reward_inputs], TD_error, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network\n",
    "Now we are ready to build up the policy $\\pi_\\theta(A|S)$ -- a network, parameterized by $\\theta$ that takes the user's current state $S$ as input and output probabilities of choosing all possible actions $\\pi(A|S)$.\n",
    "\n",
    "We are gonna re-use the state representation network to encode observations into states for us, and we connect the output of the state repersentation network to two layers: one dense layer that computes the *preferences* for each action in this state followed by a *soft-max* layer that normalizes these preferences into probabilities.\n",
    "\n",
    "The preference $h(S,A)$ for each possible action is computed through a linear combination of the state representation $h(S,A) = w_A^Tx(S)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences = Dense(num_items, name='preferences')(state_representation)\n",
    "probabilities = Softmax(name='probabilities')(preferences)\n",
    "\n",
    "entropy_factor = 0.0\n",
    "\n",
    "# A custom loss function for the policy model -- the first term is from the Actor-Critic algorithm\n",
    "# and the second term is the entropy of the output of the policy model (probabilities). See section\n",
    "# 'Training the Policy Network with One-step Transition' for the first term.\n",
    "# The second term prevents the policy's output becomes too concentrated on one (or a few) actions.\n",
    "# 1 is the entropy factor, the minus sign in front of it means low-entropy/high-concentrated prediction \n",
    "# contributes a high penalty to the overall loss, so it helps, softly, to avoid low-entropy prediction!\n",
    "# The true overall loss should also contain one more regularization term from the preferences layer (a Dense layer)\n",
    "def policy_gradient_loss_with_entropy(y_true, y_pred):\n",
    "    return K.categorical_crossentropy(y_true, y_pred) - entropy_factor * K.categorical_crossentropy(y_pred, y_pred)\n",
    "\n",
    "policy_model = Model(inputs=[action_inputs, reward_inputs], outputs=probabilities)\n",
    "policy_model.compile(loss=policy_gradient_loss_with_entropy, optimizer='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample an Action from the Policy Network\n",
    "Given a state representation $S_t$, which is encoded from observation $O_{t-1}$, we can do a forward pass to compute the probabilities of each action under current policy network (output of policy_model). Then we sample an action [1, num_items] from this probabilistic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the probabilities of taking each action under \n",
    "# current (stochastic) policy\n",
    "def predict_policy_probabilities(observation):\n",
    "    '''\n",
    "    params: observation: a list of user's current historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "    \n",
    "    return: probabilities: a numpy array of size num_items where each element corresponds to the \n",
    "                           probability of choosing that action under current policy\n",
    "    '''\n",
    "    assert len(observation) % 2 == 0, 'historical records to be encoded to a state must have even length.'\n",
    "    \n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    probabilities = policy_model.predict([action_inputs, reward_inputs])[0]\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# Take an action from the (stochastic) policy\n",
    "def sample_action_from_policy(observation, recommended_items):\n",
    "    '''\n",
    "    params: observation: a list of user's current historical records [a_1,r_1,...,a_{t-1},r_{t-1}].\n",
    "            recommended_items: a list of recommended items for current user. Used to prevent repeated recommendation.\n",
    "    \n",
    "    return: an action A_t to take according to the current policy network. (a scalar)\n",
    "    '''\n",
    "    probabilities = predict_policy_probabilities(observation)\n",
    "    # Set the probability of already recommended items to zero\n",
    "    for recommended_item in recommended_items:\n",
    "        probabilities[recommended_item - 1] = 0.0\n",
    "    # Normalize probability so that it sums up to one again\n",
    "    probabilities /= np.sum(probabilities)\n",
    "    \n",
    "    sampled_action = np.random.choice(np.arange(1, num_items+1), p=probabilities)\n",
    "    \n",
    "    return sampled_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Policy Network with One-step Transition\n",
    "According to the One-step Actor-Critic (Episodic) algorithm, the policy update formula is $$\\theta \\leftarrow \\theta + \\alpha \\gamma^t (R_t + \\gamma\\hat{v}_w(S_{t+1}) - \\hat{v}_w(S_t)) \\nabla_\\theta(\\ln\\pi_\\theta(A_t|S_t))$$\n",
    "\n",
    "We made two somewhat unreasonable modifications for practical reasons:\n",
    "1. Strictly speaking, this is an algorithm for episodic case and our recommendation problem is a continuing problem per se, and this update is for episodic problems. But nevertheless, we will still use it because the Action-Critic algorithm for continuing problem on the textbook uses average reward, something I don't think I have a good understanding. So we will still use this algorithm despite it might not be a perfect match.\n",
    "2. We will also get rid of the $\\gamma^t$ term. This changes the estimate of the gradient of the objective function ($J(\\theta)$) an biased one but we will live with it, because otherwise, as t goes up, we are afraid that the update will be scaled too small such that no substantial change will be made to the parameters $\\theta$.\n",
    "\n",
    "Thus, the update for our policy model becomes \n",
    "$$ \\delta = R_t + \\gamma\\hat{v}_w(S_{t+1}) - \\hat{v}_w(S_t) $$\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta(\\ln\\pi_\\theta(A_t|S_t))$$\n",
    "\n",
    "Then, our job is to design a loss as well as an appropriate input-output from $(S_t,A_t,R_t,S_{t+1})$ such that when keras computes the gradients of the network's parameters $\\theta$, it should equal to $\\delta \\nabla_\\theta(\\ln\\pi_\\theta(A_t|S_t))$.\n",
    "We use $(x,y) = (S_t, \\delta \\text{one_hot}(A_t))$ as our training sample (we train the network on only one sample at a time which is constructed from the latest transition) and use 'categorical_crossentropy' loss: \n",
    "$$ -\\sum_{i=1}^{|A|}y_i \\ln \\hat{y}_i = - \\sum_{i=1}^{|A|} \\delta \\text{one_hot}(A_t)_i \\ln \\pi_\\theta(A_i|S_t) = - \\delta \\ln \\pi_\\theta(A_t|S_t)$$\n",
    "\n",
    "Finally, when we call model.fit() with one training sample $(x,y)$ fed in, it will automatically compute the gradient of this loss w.r.t. $\\theta$ \n",
    "$$ \\nabla_\\theta ( - \\delta \\ln \\pi_\\theta(A_t|S_t)) = - \\delta \\nabla_\\theta (\\ln \\pi_\\theta(A_t|S_t)) $$\n",
    "and update $\\theta$ in the opposite direction of the gradient (in order to minimize the loss)\n",
    "$$ \\theta \\leftarrow \\theta - \\alpha (- \\delta \\nabla_\\theta (\\ln \\pi_\\theta(A_t|S_t))) \\\\ \\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta (\\ln \\pi_\\theta(A_t|S_t)) $$\n",
    "which is exactly what we want! Yay!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_model_with_one_step_transition(observation, action, reward):\n",
    "    '''\n",
    "    params: observation: the user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "            action: the action a_t just taken by the recommendation system (a scalar)\n",
    "            reward: user's feedback on the action/item just taken (a scalar)\n",
    "    '''\n",
    "    assert len(observation) % 2 == 0, 'historical records to be encoded to a state must have even length.'\n",
    "    \n",
    "    predicted_value_state = predict_state_value(observation)\n",
    "    predicted_value_next_state = predict_state_value(observation + [action, reward])\n",
    "    \n",
    "    TD_error = reward + gamma * predicted_value_next_state - predicted_value_state\n",
    "    \n",
    "    # create an one-hot encoding of action.\n",
    "    # Do not confused it with the hot_encoding lambda layer.\n",
    "    # Here it is just a trivial hot encoding of action.\n",
    "    one_hot_encoding_action = np.zeros(num_items).reshape(1,-1) # Shape = (1,num_items)\n",
    "    one_hot_encoding_action[0][action-1] = 1 # [action-1] because actions are indexed from 1 to num_items\n",
    "    \n",
    "    # a strange target, isn't it? See the documentation for more details!\n",
    "    target = TD_error * one_hot_encoding_action\n",
    "    \n",
    "    # retrieve inputs\n",
    "    action_inputs, reward_inputs = obs2modelinputs(observation)\n",
    "    \n",
    "    policy_model.fit([action_inputs, reward_inputs], target, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Step Actor-Critic Algorithm\n",
    "We have built all the components needed to implement the one-step Actor-Critic algorithm. We will now integrate them into one single function `one_step_actor_critic` that takes as input one transition tuple $(S_t, A_t, R_t, S_{t+1})$ (well, actually $(O_{t-1}, A_t, R_t)$ since $S_t$ can be encoded from $O_{t-1}$ and $S_{t+1}$ can be encoded from $O_t = [O_{t-1},A_t,R_t]$). This function does two things: update the state-value prediction network (Critic) and update the policy network (Actor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_actor_critic(observation, action, reward):\n",
    "    '''\n",
    "    params: observation: the user's historical records [a_1,r_1,...,a_{t-1},r_{t-1}]\n",
    "            action: the action a_t just taken by the recommendation system (a scalar)\n",
    "            reward: user's feedback r_t on the action a_t (a scalar)\n",
    "    \n",
    "    '''\n",
    "    train_state_value_prediction_model_with_one_step_transition(observation, action, reward)\n",
    "    train_policy_model_with_one_step_transition(observation, action, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Simulator\n",
    "Now we have every component done and are ready to test our model/algorithm with experiment. Since our algorithm concerns the problem of learning a good recommendation policy via interaction with the user, we will try to test how good the performance of our model/algorithm is by measuring the learning process with interactive recommendation experience with user. However, one big problem is that it is commercially impractical to test our algorithm by putting our recommendation model/algorithm online and let it interact with real customers. Therefore, we build an environment simulator that tries to mimic the customer's feedback/reward on recommended items based on public recommendation dataset. This environment simulator takes a user_id (an int) and a item_id (an int) as input and returns the user's feedback (a float) on this item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset -- MovieLens 10M\n",
    "Here we briefly discuss the dataset we used to build the environment simulator. We used the *MovieLens 10M Dataset* which contains 10,000,054 ratings for 10677 movies from 69878 users. The ratings range from 0.0 (exclusive) to 5.0 (inclusive) with an increment of 0.5. It also contains other information like tags of movies and time stamps but we will only use the ratings information (i.e. ratings for different movies from different users).\n",
    "\n",
    "For example, among all those 10,000,054 ratings, one rating is like 46::152::3.5, which stands for user_46 gave a 3.5 to movie_152. Both the index of users and movies start from 1 to 69878 and 10677 respectively. \n",
    "\n",
    "On average, each user has 143 ratings (to 143 different movies), and each movie is rated by 936 users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Up a Simulator from MovieLens 10M\n",
    "The idea is simple: we are going to create a large rating table whose rows represents user ids (from 1 to 69878) and columns represents movie ids (from 1 to 10677). If the dataset contains a rating $r$ for user_i to movie_j, then the $(i,j)$ entry of the table is $r$. We are also going to normalize the ratings from $(0.0, 5.0]$ to $(-1,1)$ while maintaining ratio:\n",
    "$$new\\_rating = \\frac{2}{5} * old\\_rating  - 1$$\n",
    "And for those entries without actual rating in the dataset, we thus set it to 0.0, implying a neutral rating in the normalized range. So, you can see that this table is in fact a large sparse table with a lot of 0.0's. Well, there is nothing we can do about this since the dataset contains only this much ratings to fill a small part of the rating table.\n",
    "\n",
    "One little extra thing is that in the original dataset, the ids of the users and movies are not indexed with {1,2,3,...} but might have gaps. That is, the actual index of users can be something like {1,2,3,5,6,9,10,...} so you might see a rating record from user_id 71567 although there are in total just 69878 users. Hence, I also reordered the user and movie index from {1,2,3,...} up 7to either 69878 for user_ids or 10677 for movie_ids through a simple mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/xiang/Desktop/Graduation_Thesis/ratings.dat'\n",
    "data = np.loadtxt(data_path, delimiter='::')\n",
    "\n",
    "rating_range_low = 0.0\n",
    "rating_range_high = 5.0\n",
    "new_rating_range_low = -1.0\n",
    "new_rating_range_high = 1.0\n",
    "# new_data = a * old_data + b transforms data from one range\n",
    "# to another, which must be symmetric around 0, while maintaining ratio\n",
    "assert new_rating_range_low + new_rating_range_high == 0, 'Normalized range must be symmetric around 0'\n",
    "a = (new_rating_range_high - new_rating_range_low) / (rating_range_high - rating_range_low)\n",
    "b = new_rating_range_low - a * rating_range_low      \n",
    "\n",
    "user_ids = set()\n",
    "movie_ids = set()\n",
    "\n",
    "for user_id, movie_id, rating, time_step in data:\n",
    "    user_ids.add(int(user_id))\n",
    "    movie_ids.add(int(movie_id))\n",
    "\n",
    "# Create the user ids mapping\n",
    "# each key-value pair is (new_index : old_index_in_data)\n",
    "user_ids_mapping = dict()\n",
    "i = 1\n",
    "for user_id in user_ids:\n",
    "    user_ids_mapping[i] = user_id\n",
    "    i += 1 \n",
    "\n",
    "# Similarly, create the movie ids mapping\n",
    "movie_ids_mapping = dict()\n",
    "i = 1\n",
    "for movie_id in movie_ids:\n",
    "    movie_ids_mapping[i] = movie_id\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# Create the rating table based on the (original) ratings\n",
    "# in the dataset\n",
    "rating_table = coo_matrix((data[:, 2], (data[:, 0].astype(int), data[:, 1].astype(int)))).todok()\n",
    "\n",
    "\n",
    "def get_rating(user_id, movie_id):\n",
    "    '''\n",
    "    params: user_id: index of the user (an int from 1 to num_users inclusively)\n",
    "            movie_id: index of the movie (an int from 1 to num_items inclusively)\n",
    "    \n",
    "    return: normalized simulated rating for this user_id to movie_id (a float b/t (-1, 1])\n",
    "    '''\n",
    "    old_rating = rating_table[user_ids_mapping[user_id], movie_ids_mapping[movie_id]]\n",
    "    \n",
    "    if old_rating == 0.0:\n",
    "        return old_rating\n",
    "    else:\n",
    "        # normalize the rating to (-1, 1]\n",
    "        return a * old_rating + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting All Together into the Dyna Architecture\n",
    "Congrats on making to this point. We finished all the crucial components and one last thing is to put them together into the Dyna architecture.\n",
    "\n",
    "This idea of Dyna architecture is model-based RL -- it constructs/learns a model of the environment dynamics and tries to learn from both real experience as well as imaginary experience generated from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Dyna algorithm with user_id for max_iterations (T) rounds\n",
    "# where within each round do the planning for max_planning_steps steps\n",
    "def Dyna(user_id=1, max_iterations=100, planning=True, max_planning_steps=100):\n",
    "    '''\n",
    "    params: user_id: the id of the user we are going to interact with in the (simulated)\n",
    "                     environment. (an int between 1 and num_users inclusively)\n",
    "            max_iterations: max number of iterations of the dyna algorithm (an int scalar)\n",
    "            max_planning_steps: max number of steps allowed in the planning step\n",
    "                                in the dyna algorithm (an int scalar)\n",
    "                                \n",
    "    return: observation: the interactive history (A_1,R_1,...,A_T,R_T), which will\n",
    "                         be stored in a replay buffer and later used to periodically\n",
    "                         retrain our user model.\n",
    "    '''\n",
    "    observation = []\n",
    "    recommended_items = []\n",
    "    \n",
    "    for t in range(max_iterations):\n",
    "        # Choose an action to take from our policy model\n",
    "        # and observe its reward\n",
    "        action = sample_action_from_policy(observation, recommended_items)\n",
    "        reward = get_rating(user_id, action)\n",
    "        # Perform One-step Actor-Critic update with the latest transition\n",
    "        one_step_actor_critic(observation, action, reward)\n",
    "        observation += [action, reward]\n",
    "        recommended_items += [action]\n",
    "        \n",
    "        if planning:\n",
    "            # Enter the planning module\n",
    "            rollout_observation = deepcopy(observation)\n",
    "            rollout_recommended_items = deepcopy(recommended_items)\n",
    "            \n",
    "            for _ in range(max_planning_steps):\n",
    "                # Choose an action to take from our policy model\n",
    "                # and query our reward prediction model for estimated reward\n",
    "                rollout_action = sample_action_from_policy(rollout_observation, rollout_recommended_items)\n",
    "                rollout_reward = predict_reward(rollout_observation, rollout_action)\n",
    "\n",
    "                # Perform One-step Actor-Critic update with the latest imaginary transition\n",
    "                one_step_actor_critic(rollout_observation, rollout_action, rollout_reward)\n",
    "\n",
    "                rollout_observation += [rollout_action, rollout_reward]\n",
    "                rollout_recommended_items += [action]\n",
    "\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "Finally, we are going to conduct some experiment with all the components we had built so far. From a high level, the experiment involves two parts:\n",
    "1. Interact with the enviroment and learn a good recommendation policy;\n",
    "2. Evaluate the learning process of our algorithm/model\n",
    "\n",
    "We will go through each of the two parts in further detailed subparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learn a Recommendation Policy through Interaction\n",
    "We can break down this task further into three steps:\n",
    "\n",
    "1. Pre-train a user model. We initialize a (random) policy $\\pi_\\theta(S,A)$ and use it to interact with the (simulated) environment and collect a set of interactive data $\\{(A_1^{(1)},R_1^{(1)},\\dots,A_T^{(1)},R_T^{(1)}), (A_1^{(2)},R_1^{(2)},\\dots,A_T^{(2)},R_T^{(2)}), \\dots, (A_1^{(K)},R_1^{(K)},\\dots,A_T^{(K)},R_T^{(K)}) \\}$, where $A_t^{(k)}$ represents the action taken in time step $t$ for user $k$ and $R_t^{(k)}$ accordingly stands for the reward/feedback from user $k$ for the recommended item $A_t^{(k)}$. We then use this dataset to train a user model. (K here is a hyperparameter)\n",
    "\n",
    "2. Run the Dyna algorithm to learn a good recommendation policy. Start by randomly picking up an user and run the dyna algorithm with this user (`user_id`) for several steps (`max_iterations`, `max_planning_steps`). Add the return value (a list of (real) interactive history of this user) to a replay buffer, which is a fixed length deque of lists. Then we repeat the whole process by randomly selecting another user. We repeat this process until `buffer_ratio_retrain` ((0, 1] e.g. 1/3) data in the replay buffer is replaced by the new ones. Then we go to step 3.\n",
    "\n",
    "3. Periodically retrain user model. Recall that we maintain a replay buffer (of fixed size) of interactive experiences. We will use the data in this buffer to retrain our user model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_user_model_from_replay_buffer(replay_buffer):\n",
    "    '''\n",
    "    params: replay_buffer: a queue of interactive experience of same length.\n",
    "    '''\n",
    "    \n",
    "    # Check for precondition -- make sure all elements in replay_buffer have the same length\n",
    "    if len(replay_buffer) != 0:\n",
    "        for idx in range(1, len(replay_buffer)):\n",
    "            if len(replay_buffer[idx]) != len(replay_buffer[0]):\n",
    "                assert False, 'historical experience in replay_buffer must have same length!'\n",
    "    \n",
    "    length_episode = len(replay_buffer[0]) // 2\n",
    "    \n",
    "    # Randomly select observations from replay_buffer without repetition\n",
    "    # and train our user model on each observation. Since we don't want to\n",
    "    # modify replay_buffer in-place, we create an auxiliary list for replay_buffer\n",
    "    aux_replay_buffer = list(replay_buffer)\n",
    "    np.random.shuffle(aux_replay_buffer)\n",
    "    \n",
    "    for ob in aux_replay_buffer:       \n",
    "        train_reward_prediction_model(ob, steps_per_epoch=length_episode, epochs=1)\n",
    "    \n",
    "\n",
    "# Pre-train our user model by interacting with all users using\n",
    "# an initial policy. This function does two things: fill in a \n",
    "# replay buffer and train our user model (aka reward prediction model)\n",
    "# with data in the replay buffer\n",
    "def pre_train_user_model(num_users_to_interact, length_episode, num_rounds=1):\n",
    "    '''\n",
    "    params: num_users_to_interact: number of users we are going to interact with (an int >= 1)\n",
    "            length_episode: length of the interaction with each user (an int >= 1)\n",
    "            num_rounds: number of rounds to fill in the replay buffer and train on it.\n",
    "            \n",
    "    return: a queue of length num_users_to_interact where each element is the interactive \n",
    "            history of that user.\n",
    "    '''\n",
    "    replay_buffer = deque(maxlen=num_users_to_interact)\n",
    "    \n",
    "    for idx in range(num_rounds):\n",
    "        # Fill in the replay buffer\n",
    "        for _ in range(num_users_to_interact):\n",
    "            # Randomly select an user. Note that we index users from 1 to num_users!\n",
    "            user_id = np.random.randint(num_users) + 1\n",
    "\n",
    "            # Use the initial policy to interact with user user_id for length_episode\n",
    "            # steps without improving the policy or anything -- just collect experiences\n",
    "            ob = []\n",
    "            recommended_items = []\n",
    "            \n",
    "            for _ in range(length_episode):\n",
    "                action = sample_action_from_policy(ob, recommended_items)\n",
    "                reward = get_rating(user_id, action)\n",
    "                ob += [action, reward]\n",
    "                recommended_items += [action]\n",
    "\n",
    "            # Add this user's experience (ob) to replay buffer\n",
    "            replay_buffer.append(ob)\n",
    "    \n",
    "        # Train user model from the replay buffer\n",
    "        train_user_model_from_replay_buffer(replay_buffer)\n",
    "        print('Finshed ' + str(idx) + '/' + str(num_rounds) + ' round of the pre-training step')\n",
    "    print('\\nFinished pre-training on replay buffer!')\n",
    "    \n",
    "    return replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Evaluate the Learning Process\n",
    "Every once in a while, we need to evaluate our (recommendation) policy to see how good it current is. We are going to implement a procedure which can measure the performance of our policy using four metrics -- average reward, precision@k, recall@k, and F1@k.\n",
    "\n",
    "Concretely, for **average reward**, here is how we are going to do it:\n",
    "1. Randomly select an user $i$ from 1 to num_users;\n",
    "2. Interact with this user (in the simulator) for certain steps (episode length e.g. 32) using current policy and record the rewards $R_1, R_2,\\dots,R_{T}$;\n",
    "3. Average of these $T$ rewards to calculate the average reward $\\bar{R}^{(i)}$ for this user under current policy;\n",
    "4. Go back to step 1 and repeat for multiple users, and average over all these average rewards $\\bar{R}^{(i)}$'s to get the overall average reward $\\bar{R}$.\n",
    "\n",
    "For **precision@k** (we set k=32, same as episode length), here is how we are going to do it:\n",
    "1. Randomly select an user $i$ from 1 to num_users;\n",
    "2. Interact with this user (in the simulator) for certain steps (episode length e.g. 32) using current policy. \n",
    "   - During each step, look at the top-k (top-32) actions/items with highest probabilities. Then query the environment simulator to see how many of them have a reward of >= 0.2 (0.2 is the chosen threshould that corresponds 3 stars in our example). Then the proportion (e.g. 1/32 if only one of the 32 items with highest probabilities has an actual reward of >= 0.2) is the precision@k for this step. \n",
    "   - Repeat for 32 steps to compute 32 precision@k's and average over them to get the precision@k for this user.\n",
    "3. Go back to step 1 and repeat for multiple users, and average over all these precision@k to get the overall precision@k.\n",
    "\n",
    "For **recall@k** (k=32), this is how we are going to do it:\n",
    "1. Randomly select an user $i$ from 1 to num_users;\n",
    "2. Query the environment simulator for the reward about ALL the actions (from 1 to num_items) on this user $i$. Record those actions with a reward >= 0.2 (the same threshould as in precision@k); (minor note: If there is no such action (no relevant items), which I don't think will happen in this dataset, then the recall@k is set to 1 and done. No need to go to step 3.)\n",
    "3. Interact with this user (in the simulator) for certain steps (e.g. 32) using current policy.\n",
    "    - During each step, look at the top-k (top-32) actions/items with highest probabilities. Then query the environment simulator to see how many of them have a reward of >= 0.2 (0.2 is the chosen threshould that corresponds 3 stars in our example). Then the proportion (e.g. 1/134 if only one of the top-32 items with highest probabilities has an actual reward of >= 0.2 when 134 means this user $i$ has 134 relevant ratings) is the recall@k for this step.\n",
    "    - Repeat for 32 steps to compute 32 recall@k's and average over them to get the recall@k for this user.\n",
    "4. Go back to step 1 and repeat for multiple users, and average over all these recall@k to get the overall recall@k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_current_policy(threshould, num_users_to_interact, length_episode=32, k=32):\n",
    "    '''\n",
    "    params: threshould: threshould of reward/rating to distinguish relevant and recommended actions.\n",
    "                        Must be in normalized scale!!!\n",
    "            num_users_to_interact: number of users we will interact to average out the noiseness (an int >= 1)\n",
    "            length_episode: number of steps allowed for interaction for each user (an int >= 1)\n",
    "            k: k as used in precision@k, recall@k, and f1@k (an int >= 1)\n",
    "    \n",
    "    return: overall_average_reward (a float)\n",
    "            overall_precision_at_k (a float)\n",
    "            overall_recall_at_k (a folat)\n",
    "            overall_f1_at_k (a float)\n",
    "    '''\n",
    "    # check pre-conditions\n",
    "    assert 0 < threshould <= new_rating_range_high, 'Input threshould must be in normalized scale!!!'\n",
    "    assert num_users_to_interact >= 1\n",
    "    assert length_episode >= 1\n",
    "    assert k >= 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    overall_average_reward = 0.\n",
    "    overall_precision_at_k = 0.\n",
    "    overall_recall_at_k = 0.\n",
    "    overall_f1_at_k = 0.\n",
    "    \n",
    "    for idx in range(num_users_to_interact):\n",
    "        # Randomly select an user. Note that we index users from 1 to num_users!\n",
    "        user_id = np.random.randint(num_users) + 1\n",
    "        \n",
    "        # Average metrics over multiple interaction steps\n",
    "        average_reward = 0.\n",
    "        precision_at_k = 0.\n",
    "        recall_at_k = 0.\n",
    "        f1_at_k = 0.\n",
    "        \n",
    "        num_relevant_actions = 0\n",
    "        for action_id in range(1, num_items+1):\n",
    "            if get_rating(user_id, action_id) >= threshould:\n",
    "                num_relevant_actions += 1\n",
    "        if num_relevant_actions == 0:\n",
    "            recall_at_k = 1.\n",
    "            print('Oops, be careful because user No.' + str(user_id) + ' does not have any relevant ratings!')\n",
    "        \n",
    "        \n",
    "        ob = []\n",
    "        recommended_items = []\n",
    "\n",
    "        for t in range(length_episode):\n",
    "            # Compute average reward in an incremental way over multiple steps\n",
    "            action = sample_action_from_policy(ob, recommended_items)\n",
    "            reward = get_rating(user_id, action)\n",
    "            average_reward += 1/(t+1) * (reward - average_reward)\n",
    "            \n",
    "            \n",
    "            # Compute recommended actions (aka the k actions with highest probabilities)\n",
    "            probabilities = predict_policy_probabilities(ob)\n",
    "            # Pick out k actions/items with highest probabilities\n",
    "            actions_with_ascending_probability = np.argsort(probabilities)\n",
    "            recommended_actions = actions_with_ascending_probability[-k:] + 1 # remember that our actions are indexed from 1 to num_items!\n",
    "            \n",
    "            # number of recommended AND relevant actions, meaning how many actions (out the k actions)\n",
    "            # also have a true rating >= threshould\n",
    "            num_recommended_relevant_actions = 0 \n",
    "            for action_id in recommended_actions:\n",
    "                if get_rating(user_id, action_id) >= threshould:\n",
    "                    num_recommended_relevant_actions += 1\n",
    "                    \n",
    "            # Compute average precision@k in an incremental way over multiple steps\n",
    "            precision_at_k_this_step = num_recommended_relevant_actions / k\n",
    "            precision_at_k += 1/(t+1) * (precision_at_k_this_step - precision_at_k)\n",
    "            \n",
    "            # Compute average recall@k in an incremental way over multiple steps\n",
    "            recall_at_k_this_step = 1. # in case num_relevant_actions is 0\n",
    "            if num_relevant_actions != 0:\n",
    "                recall_at_k_this_step = num_recommended_relevant_actions / num_relevant_actions\n",
    "                recall_at_k += 1/(t+1) * (recall_at_k_this_step - recall_at_k)\n",
    "            \n",
    "            # Compute average f1@k in an incremental way over multiple steps\n",
    "            if (precision_at_k_this_step + recall_at_k_this_step) != 0.:\n",
    "                f1_at_k_this_step = 2 * precision_at_k_this_step * recall_at_k_this_step / (precision_at_k_this_step + recall_at_k_this_step)\n",
    "            else:\n",
    "                f1_at_k_this_step = 0.\n",
    "            f1_at_k += 1/(t+1) * (f1_at_k_this_step - f1_at_k)\n",
    "            \n",
    "            ob += [action, reward]\n",
    "            recommended_items += [action]\n",
    "        \n",
    "        \n",
    "        overall_average_reward += 1/(idx+1) * (average_reward - overall_average_reward)\n",
    "        overall_precision_at_k += 1/(idx+1) * (precision_at_k - overall_precision_at_k)\n",
    "        overall_recall_at_k += 1/(idx+1) * (recall_at_k - overall_recall_at_k)\n",
    "        overall_f1_at_k += 1/(idx+1) * (f1_at_k - overall_f1_at_k)\n",
    "        \n",
    "    return overall_average_reward, overall_precision_at_k, overall_recall_at_k, overall_f1_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Final Experiment with Evaluation\n",
    "\n",
    "Todo: Finish this cell (simply the documentation), add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(filename_evaluation_data, directoryname_policy_weights, max_rounds, replay_buffer, buffer_ratio_retrain, length_episode, planning, max_planning_steps, threshould):\n",
    "    '''\n",
    "    params: filename_evaluation_data: the path of the file that will store all the evaluation \n",
    "                                        information (a string e.g. \"data/model_free_solution/entropy_0.0\").\n",
    "            directoryname_policy_weights: the path of the directory that will store the weights of the \n",
    "                                        policy at each iteration (a string e.g. \"weights/model_free_solution/entropy_0.0/\").\n",
    "            max_rounds: number of rounds for the training process -- one round stands for interacting and \n",
    "                        learning from multiple users and retrain user model once.\n",
    "            replay_buffer: a fixed-length queue with elements being interactive experience from \n",
    "                           various user with the environment. Or, simply use the return of the\n",
    "                           pre_train_user_model function. (a fixed-length deque);\n",
    "            buffer_ratio_retrain: periodically retrain our user model (aka reward prediction model) when this proportion\n",
    "                                  of replay buffer has been replaced with new interactive data (a float in (0,1])\n",
    "            length_episode: number of (real) steps allowed for each user for interaction in the dyna algorithm (an int >=1);\n",
    "            planning: whether we want to do planning or not. If not, max_planning_steps have no effect. (a boolean) \n",
    "            max_planning_steps: number of steps for planning for each user in the dyna algorithm (an int >=1).\n",
    "    '''\n",
    "    # check pre-conditions\n",
    "    # 1. make sure it is a valid replay buffer\n",
    "    if len(replay_buffer) == 0:\n",
    "        assert False, 'Please fill in replay buffer first!'\n",
    "    else:\n",
    "        for idx in range(1, len(replay_buffer)):\n",
    "            if len(replay_buffer[idx]) != len(replay_buffer[0]):\n",
    "                assert False, 'historical experience in replay_buffer must have same length!'\n",
    "    \n",
    "    # 2. make sure length_episode is equal to the length of existing element in replay_buffer\n",
    "    assert length_episode == (len(replay_buffer[0]) / 2), 'length_episode must have the same length as experiences in replay_buffer'\n",
    "    \n",
    "    \n",
    "    evaluation_data = ''\n",
    "    periodically_retrain_steps = int(buffer_ratio_retrain * len(replay_buffer))\n",
    "\n",
    "    for idx in range(1, max_rounds+1):\n",
    "        print('\\n\\n' + str(idx) + 'th round of the main training process:')\n",
    "\n",
    "        # Interact with multiple users (determined by periodically_retrain_steps)\n",
    "        # and learn from them in a model-based manner\n",
    "        for t in range(periodically_retrain_steps):\n",
    "            user_id = np.random.randint(num_users) + 1\n",
    "\n",
    "            ob = Dyna(user_id=user_id, max_iterations=length_episode, planning=planning, max_planning_steps=max_planning_steps)\n",
    "\n",
    "            replay_buffer.append(ob)\n",
    "\n",
    "            if (t+1)%100 == 0 or t == 0:\n",
    "                print('    interaction with ' + str(t) + 'th user in this round finished')\n",
    "        \n",
    "        if planning:\n",
    "            print('Retrain reward prediction model for the ' + str(idx) + 'th round.')\n",
    "            # Periodically retrain user model from (updated) replay buffer\n",
    "            train_user_model_from_replay_buffer(replay_buffer)\n",
    "            print('Retraining reward prediction model is finished!')\n",
    "\n",
    "        # Evaluate current policy\n",
    "        average_reward, precision_at_k, recall_at_k, f1_at_k = evaluate_current_policy(threshould=threshould, num_users_to_interact=len(replay_buffer), length_episode=length_episode, k=length_episode)\n",
    "        print('Performance of current policy:\\n' + str(average_reward) + '::' + str(precision_at_k) + '::' + str(recall_at_k) + '::' + str(f1_at_k) + '\\n')\n",
    "        \n",
    "        # Record evaluation of current policy\n",
    "        evaluation_data += str(average_reward) + \"::\" + str(precision_at_k) + \"::\" + str(recall_at_k) + \"::\" + str(f1_at_k) + \"\\n\"\n",
    "        \n",
    "        # Save weights of the (recommendation) policy network\n",
    "        policy_model.save_weights(directoryname_policy_weights + 'policy_weights_at_iteration_' + str(idx) + '.h5')\n",
    "    \n",
    "    # Log evaluation information to a file\n",
    "    with open(filename_evaluation_data, 'w') as file:\n",
    "        file.write(evaluation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both 4a and 4b use saved weights for the state representation network, gained by a pre-train step. Thus, we should always start our experiment by loading the saved weights to the state representation network and fill in the replay buffer with arbitrary episodes (why? see doc for function ``fill_replay_buffer_random_episodes``). \n",
    "\n",
    "If you insist on doing the pre-train step, you need to run the code twice:\n",
    "- Comment out a freeze-weight-code snippet in the \"reward prediction model\" section. This will cause all the layers to be trainable. Run the pre-train step *only* to update the state representation model's weights through back-prop in the reward prediction model. Notice that the pre-training step will only affect weights of the reward prediction model but not on the other two models. Then save the weights of the state representation model.\n",
    "- Uncomment the freeze-weight-code snippet in the \"reward prediction model\" section so that the state representation model will remain unchanged no matter which one of the three models we train. Then load the saved weights of the reward prediction model and run the code!\n",
    "    \n",
    "\n",
    "#### Sanity Check 4a (model-free solution)\n",
    "This sanity check is the model-free solution. Provided with a initial state representation weights, load it and fill in replay buffer with random episodes. Then we can run the main_train function. For efficiency concers, in the model-free solution, since we don't need to periodically retrain the model, I will just comment out the corresponding line of code in the main_train function. Remember to uncomment it for sanity check 4b (the model-based solution)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1th round of the main training process:\n",
      "    interaction with 0th user in this round finished\n",
      "    interaction with 99th user in this round finished\n",
      "    interaction with 199th user in this round finished\n",
      "    interaction with 299th user in this round finished\n",
      "    interaction with 399th user in this round finished\n",
      "    interaction with 499th user in this round finished\n",
      "    interaction with 599th user in this round finished\n",
      "    interaction with 699th user in this round finished\n",
      "    interaction with 799th user in this round finished\n",
      "    interaction with 899th user in this round finished\n",
      "    interaction with 999th user in this round finished\n",
      "    interaction with 1099th user in this round finished\n",
      "    interaction with 1199th user in this round finished\n",
      "    interaction with 1299th user in this round finished\n",
      "    interaction with 1399th user in this round finished\n",
      "    interaction with 1499th user in this round finished\n",
      "    interaction with 1599th user in this round finished\n",
      "    interaction with 1699th user in this round finished\n",
      "    interaction with 1799th user in this round finished\n"
     ]
    }
   ],
   "source": [
    "def fill_replay_buffer_random_episodes(replay_buffer_size, length_episode):\n",
    "    # We don't have pre-train here to return a full replay buffer for us,\n",
    "    # which is required in the main function. We will just create one and stuff \n",
    "    # it with replay_buffer_size meaningless lists. Those replay_buffer_size lists\n",
    "    # will be replaced by new interactive data in the first round before re-training\n",
    "    # the user model on the replay buffer, so this is okay!\n",
    "    replay_buffer = deque(maxlen=replay_buffer_size)\n",
    "    for _ in range(replay_buffer_size):\n",
    "        fake_ob = []\n",
    "        for _ in range(length_episode):\n",
    "            fake_ob += [0,0]\n",
    "        replay_buffer.append(fake_ob)\n",
    "    return replay_buffer\n",
    "\n",
    "\n",
    "filename_evaluation_data = './data/model_based_solution/masking_entropy_0.0'\n",
    "directoryname_policy_weights = './weights/model_based_solution/masking_entropy_0.0/'\n",
    "max_rounds = 30 # The number of rounds for this program (each replay_buffer_size*buffer_ratio_retrain iterations counts as one round)\n",
    "replay_buffer_size = 3000 # The number of user interactions in the replay buffer\n",
    "buffer_ratio_retrain = 1.0 # If this ratio of user interactions in the replay buffer have been replaced by new interactions, retrain the user model\n",
    "length_episode = 32 # The number of steps allowed for real interaction for each user\n",
    "planning = True # whether or not to use planning when interacting with each user\n",
    "max_planning_steps = 5 # If planning, this many steps of planning are allowed after each step of real interaction\n",
    "threshould = 0.19 # The threshould of normalized reward above which are considered relevant items. Used in computing precision@n, recall@n,and f1@n when evaluating the policy.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# # Pre-train the user model (state representation model + reward prediction model)\n",
    "# replay_buffer = pre_train_user_model(num_users_to_interact=replay_buffer_size, length_episode=length_episode, num_rounds=23)\n",
    "# print('Time to pre-train the user model is ' + str((time.time() - start_time) / 60) + ' minutes')\n",
    "# print('Save the weights of the user model to saved_weights_pretrained_user_model.h5')\n",
    "# reward_prediction_model.save_weights('saved_weights_pretrained_user_model.h5')\n",
    "\n",
    "reward_prediction_model.load_weights('saved_weights_pretrained_user_model.h5')\n",
    "replay_buffer = fill_replay_buffer_random_episodes(replay_buffer_size, length_episode)\n",
    "\n",
    "main_train(filename_evaluation_data=filename_evaluation_data, directoryname_policy_weights=directoryname_policy_weights, max_rounds=max_rounds, replay_buffer=replay_buffer, buffer_ratio_retrain=buffer_ratio_retrain, length_episode=length_episode, planning=planning, max_planning_steps=max_planning_steps, threshould=threshould)\n",
    "\n",
    "print('\\n\\nTotal time to run the program is ' + str((time.time() - start_time) / 60) + ' minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining Issues:\n",
    "\n",
    "~~`Question:`~~ When updating the policy network parameters, do we update the state representation model as well?\n",
    "\n",
    "~~`Question:`~~ Similarly, when we update the state-value prediction network, do we update the state representation model?\n",
    "\n",
    "**Answer:** it is unknown. Maybe try both! Currently we don't.\n",
    "    \n",
    "~~`Question:`~~ What about the discounting factor $\\gamma$? Do we use it in our update?\n",
    "\n",
    "**Answer:** Don't use it at this moment. Maybe Try it if I have time! \n",
    "    \n",
    "`Question:` When t goes up, the user's historical records increases (ob becomes a long long list), which affects the speed of computing many functions needed to update the policy. Is it reasonable for us to keep track of only the most recent K records as our observation?\n",
    "\n",
    "**Answer:** No, since the observation sequence won't get too long.    \n",
    "\n",
    "~~`Question:`~~ When we fill in the replay buffer with interactive experience with the user, a lot of (simulated) rewards are 0 not because they're actually 0 but rather because they are unknown and we artificially set it to 0. Then when we use the replay buffer to train our user model (the reward prediction model), we are forcing the model to learn that the rewards for those actions are 0's, which might not be the case. We can try to use only experiences with real ratings to train the user model, but I am now not sure how to do it. Anyway, let's just do the former solution first and see what happens.\n",
    "\n",
    "~~`Question:`~~ Parameter tuning\n",
    "\n",
    "~~`Question:`~~ case study to understand its behavior\n",
    "\n",
    "`Question:` User model overfitting (poor generalization?)\n",
    "\n",
    "~~`Question:`~~ Try multi-step methods or eligibility trace for the policy-gradient method.\n",
    "\n",
    "~~`Question:`~~ What about a model-free solution but with experience replay?\n",
    "\n",
    "`Question:` $R_0$ might should be 2.5 rather than 0, because after normalization, $R_0$ becomes -1.\n",
    "\n",
    "~~`Question:`~~ Are precision@n, recall@n, f1@n really reasonable evaluation metrics?\n",
    "\n",
    "**Answer:** precision@n: what if some user has just a few (say < n) relevant items, then precision@n for this user will never be able to get close to 1 no matter how good the recommendation policy is.\n",
    "\n",
    "recall@n: what if some user has NO relevant items (denominator is 0), should we set the recall@n for this user to 1, as we do in conventional recommendation systems? Also, the denominator is related to number of relevant items of the user, rather than related to n, so the range of recall@n is no longer [0,1] depending on the quality of the recommendation policy. Thus, I personally think it is no longer suitable metrics for our problem.\n",
    "\n",
    "f1@n: if recall@n does not make sense in the first place, how can f1@n, one quantity that relies on precision@n and recall@n, make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "0. Change the way of logging data: Instead of printing the evaluation out in readable format, save it to a file.\n",
    "\n",
    "1. Understand the behavior of the recommendation system: \n",
    "    - re-run the model-free algorithm in the exactly the same way (no pre-train this time, just load the saved weights of the state representation model) as before but when evaluating the recommendation policy, we also save the weights of the policy network. Then after running the algorithm, do two things: 1). Check whether data is reproducible; 2) look at the average reward plot and see at which iteration(s) the average reward experiences the suddence change and go back to the code. Load the corresponding weights of the policy model as well as the state representation model. Interact with a few users (say, 3 or 4) for a few steps (say 32) to see what items this policy is recommending.\n",
    "    \n",
    "    - \n",
    "\n",
    "2. (Hyper)parameter study:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Diagnosis\n",
    "Many strange things happened in the result. We want to understand the behavior of the algorithms more so we are going to try to print out some intermediate result, plot some figures, etc, to try to understand the result and see how we can improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-free Solution\n",
    "### 1. Print out what items it is recommending at different stages\n",
    "To start with, let's try to print out the items the policy is recommending as well as the top-10 items with their probabilities.\n",
    "\n",
    "Let me just describe my observations on the behavior of the policy:\n",
    "- On the **early stage, middle stage, and late stage** of the learning process (end of iteration 1, 3000 users interaction): \n",
    "    1. Probabilities of recommended items: the items with highest (top-32) probabilities are pretty much the same as well as their probabilities of being selected, regardless of the historical records. This happens both within the same user and across different users.\n",
    "    2. Every action has (almost) equal probability of being selected. So random recommendations are expected and most of them result in a zero reward. Thus, we expect the interactive experience sequence to be something like $[2202, 0.0, 3888, 0.0, 5374, 0.0, 7371, 0.0, 420, 0.0, 8535, 0.0, 6703, 0.0, 873, 0.0, 9327, 0.0]$, user_id = 16908.\n",
    "    3. For each user, the precision@32 at each time step remains (almost) the same acroos the 32(T) steps and not large. It varies b/t different users, mainly simply because different users have different ratings on those actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy_model.load_weights('weights/model_free_solution/policy_weights_at_iteration_30.h5')\n",
    "\n",
    "for _ in range(3):\n",
    "    user_id = np.random.randint(num_users) + 1\n",
    "    print(\"\\n\\n\\n\\nInteract with user No.\" + str(user_id))\n",
    "    \n",
    "    # Interac for 10 steps\n",
    "    ob = []\n",
    "    recommended_items = []\n",
    "    \n",
    "    for t in range(10):\n",
    "        print('When state = ' + str(ob) + ', actions with highest 32 probabilities = ')\n",
    "        probs = predict_policy_probabilities(ob)\n",
    "        num_relevant_recommended_items = 0\n",
    "        for i in reversed(probs.argsort()[-32:]):\n",
    "            print('prob = ' + str(probs[i]) + ' action = ' + str(i+1) + ' reward (if taken) = ' + str(get_rating(user_id, i+1)))\n",
    "            if get_rating(user_id, i+1) >= 0.19:\n",
    "                num_relevant_recommended_items += 1\n",
    "        print('precision@32 for this step = ' + str(num_relevant_recommended_items/32))\n",
    "                \n",
    "        # Select an action\n",
    "        action = sample_action_from_policy(ob, recommended_items)\n",
    "        print('We select action ' + str(action) + 'from those probabilities')\n",
    "        # Take the action and observe reward\n",
    "        reward = get_rating(user_id, action)\n",
    "        print('We receive reward of ' + str(reward) + ' after taking this action\\n')\n",
    "\n",
    "        ob += [action, reward]\n",
    "        recommended_items += [action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further confirm our first observation (policy cannot distinguish users from historical records), I will just plot a figure that quantifies the overlapping degree of the top-32 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the overlapping degree b/t two dictionaries\n",
    "# where the keys are the actions and the values are their\n",
    "# corresponding probabilities.\n",
    "def compute_overlapping(l1, l2):\n",
    "    '''\n",
    "    params: l1: a dictionary of action-probability pairs\n",
    "            l2: a dictionary of action-probability pairs (must have the same number of pairs as l1)\n",
    "            \n",
    "    return: overlapping_ratio: the proportion of same actions in both dictionaries. It ranges b/t [0,1] where\n",
    "                                0 means least overlapped and 1 means totally overlapped.\n",
    "            overlapping_degree: among those shared actions, how much do they differ from each other. It ranges from\n",
    "                                (0,1] where 0 means those (same) actions actually have very different probabilities and\n",
    "                                1 means those (same) actions have exactly the same probablity too!\n",
    "            \n",
    "    '''\n",
    "    assert len(l1) == len(l2), 'input dictionaries must have equal length.'\n",
    "    \n",
    "    n = len(l1)\n",
    "    # A list that stores the overlapped actions\n",
    "    overlapped_actions = []\n",
    "    \n",
    "    for action in l1:\n",
    "        if action in l2:\n",
    "            overlapped_actions.append(action)\n",
    "    \n",
    "    overlapping_ratio = len(overlapped_actions) / len(l1)\n",
    "    \n",
    "    overlapping_degree = 1.0\n",
    "    for overlapped_action in overlapped_actions:\n",
    "        overlapping_degree *= np.exp(-np.abs(l1[overlapped_action] - l2[overlapped_action]))\n",
    "    \n",
    "    return overlapping_ratio, overlapping_degree\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate overlapping on recommended items of\n",
    "# the policy at certain iteration.\n",
    "def empirically_evaluate_overlapping(iteration):\n",
    "    '''\n",
    "    params: iteration: Number of iteration at which the overlapping evaluation\n",
    "                       is conducted (a int b/t 1 and max_rounds).\n",
    "    return: average_overlapping_ratio_within_same_user: see their meaning in the comments\n",
    "            average_overlapping_degree_within_same_user\n",
    "            average_overlapping_ratio_across_different_users\n",
    "            average_overlapping_degree_across_different_users\n",
    "            \n",
    "    '''\n",
    "    \n",
    "    policy_model.load_weights('weights/model_free_solution/policy_weights_at_iteration_' + str(iteration) + '.h5')\n",
    "    \n",
    "    # Within same user, record the top-n recommended items at the step one (no historical records)\n",
    "    # and the last step T (a long historical records). Then compute the overlapping ratio and degree \n",
    "    # (see the function for detailed explanation) b/t them. Do the same thing for multiple users to get \n",
    "    # average overlapping score (ration and degree) within the same user. This number can reflect how the \n",
    "    # policy responds to the historical records -- become specialized policy for each user indicated by the historical records.\n",
    "\n",
    "    average_overlapping_ratio_within_same_user = 0.0\n",
    "    average_overlapping_degree_within_same_user = 0.0\n",
    "\n",
    "    for idx in range(100):\n",
    "        user_id = np.random.randint(num_users) + 1\n",
    "\n",
    "        l1 = dict()\n",
    "        l32 = dict()\n",
    "\n",
    "        ob = []\n",
    "        recommended_items = []\n",
    "        \n",
    "        for t in range(1, 33):\n",
    "            if t == 1:\n",
    "                probs = predict_policy_probabilities(ob)\n",
    "                for i in reversed(probs.argsort()[-32:]):\n",
    "                    l1[i+1] = probs[i]\n",
    "            elif t == 32:\n",
    "                probs = predict_policy_probabilities(ob)\n",
    "                for i in reversed(probs.argsort()[-32:]):\n",
    "                    l32[i+1] = probs[i]\n",
    "\n",
    "\n",
    "            action = sample_action_from_policy(ob, recommended_items)\n",
    "            reward = get_rating(user_id, action)\n",
    "            ob += [action, reward]  \n",
    "            recommended_items += [action]\n",
    "\n",
    "        overlapping_ratio, overlapping_degree = compute_overlapping(l1, l32)\n",
    "\n",
    "        average_overlapping_ratio_within_same_user += 1/(idx+1) * (overlapping_ratio - average_overlapping_ratio_within_same_user)\n",
    "        average_overlapping_degree_within_same_user += 1/(idx+1) * (overlapping_degree - average_overlapping_degree_within_same_user)\n",
    "\n",
    "\n",
    "    # Now compute a similar quantity (well, actually 2) across different users\n",
    "    # Namely, compute the top-n actions for one user at the final T time step as well\n",
    "    # as their probabilities. Do the same for a different user. Compute the overlapping\n",
    "    # degree b/t those two dictionaries. Then repeat the same thing for multiple double-users\n",
    "    # to get an average result. The result is an useful metric to measure how much the policy\n",
    "    # changes/repondes w.r.t the historical records. That is, can the policy distinguish different\n",
    "    # users from the historical records and therefore recommends differently? If the result is high\n",
    "    # (close to 1), then it means No, it cannot. If it is low, unfortunately, it doesn't mean that \n",
    "    # the policy can distinguish different users. More experiments are required in this case.\n",
    "    average_overlapping_ratio_across_different_users = 0.0\n",
    "    average_overlapping_degree_across_different_users = 0.0\n",
    "\n",
    "    for idx in range(100):\n",
    "        # Randomly select two different users\n",
    "        user_id_1 = np.random.randint(num_users) + 1\n",
    "        user_id_2 = np.random.randint(num_users) + 1\n",
    "        while user_id_2 == user_id_1:\n",
    "            user_id_2 = np.random.randint(num_users) + 1\n",
    "\n",
    "        l32_user_1 = dict()\n",
    "        l32_user_2 = dict()\n",
    "\n",
    "        # Interact with user 1 for 32 steps\n",
    "        ob = []\n",
    "        recommended_items = []\n",
    "        \n",
    "        for t in range(1, 33):\n",
    "            if t == 32:\n",
    "                probs = predict_policy_probabilities(ob)\n",
    "                for i in reversed(probs.argsort()[-32:]):\n",
    "                    l32_user_1[i+1] = probs[i]\n",
    "\n",
    "            action = sample_action_from_policy(ob, recommended_items)\n",
    "            reward = get_rating(user_id_1, action)\n",
    "            ob += [action, reward]  \n",
    "            recommended_items += [action]\n",
    "\n",
    "        # Interact with user 2 for 32 steps\n",
    "        ob = []\n",
    "        recommended_items = []\n",
    "        for t in range(1, 33):\n",
    "            if t == 32:\n",
    "                probs = predict_policy_probabilities(ob)\n",
    "                for i in reversed(probs.argsort()[-32:]):\n",
    "                    l32_user_2[i+1] = probs[i]\n",
    "\n",
    "            action = sample_action_from_policy(ob, recommended_items)\n",
    "            reward = get_rating(user_id_2, action)\n",
    "            ob += [action, reward]\n",
    "            recommended_items += [action]\n",
    "\n",
    "        # Compute overlapping degree\n",
    "        overlapping_ratio, overlapping_degree = compute_overlapping(l32_user_1, l32_user_2)\n",
    "\n",
    "        average_overlapping_ratio_across_different_users += 1/(idx+1) * (overlapping_ratio - average_overlapping_ratio_across_different_users)\n",
    "        average_overlapping_degree_across_different_users += 1/(idx+1) * (overlapping_degree - average_overlapping_degree_across_different_users)\n",
    "\n",
    "    return round(average_overlapping_ratio_within_same_user,4), \\\n",
    "        round(average_overlapping_degree_within_same_user,4), \\\n",
    "        round(average_overlapping_ratio_across_different_users,4) \\\n",
    "        round(average_overlapping_degree_across_different_users,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,a2,a3,a4 = empirically_evaluate_overlapping(1)\n",
    "b1,b2,b3,b4 = empirically_evaluate_overlapping(1)\n",
    "c1,c2,c3,c4 = empirically_evaluate_overlapping(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
